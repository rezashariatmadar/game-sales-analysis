
    <!DOCTYPE html>
    <html>
    <head>
        <title>Model Hyperparameter Tuning Report (Randomized Search)</title>
        <style>
            body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
            h1, h2, h3 { color: #2c3e50; }
            table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            th { background-color: #f2f2f2; }
            tr:nth-child(even) { background-color: #f9f9f9; }
            .metric { font-weight: bold; color: #2980b9; }
            .model-section { background-color: #f8f9fa; padding: 15px; margin-bottom: 20px; border-radius: 5px; }
            .parameter { font-family: monospace; background-color: #f0f0f0; padding: 2px 4px; }
            img { max-width: 100%; height: auto; }
        </style>
    </head>
    <body>
        <h1>Model Hyperparameter Tuning Report (Randomized Search)</h1>
        <p>Generated on: 2025-05-31 12:55:01</p>
        
        <div class="model-section">
            <h2>Random Forest Regression Model</h2>
            <h3>Best Parameters</h3>
            <table>
                <tr><th>Parameter</th><th>Value</th></tr>
    <tr><td>bootstrap</td><td>True</td></tr><tr><td>max_depth</td><td>45</td></tr><tr><td>max_features</td><td>None</td></tr><tr><td>min_samples_leaf</td><td>4</td></tr><tr><td>min_samples_split</td><td>4</td></tr><tr><td>n_estimators</td><td>196</td></tr>
            </table>
            
            <h3>Performance Metrics</h3>
            <ul>
                <li>Mean Squared Error: <span class="metric">0.0181</span></li>
                <li>R² Score: <span class="metric">0.9732</span></li>
                <li>Tuning Time: <span class="metric">62.20 seconds</span></li>
            </ul>
            
            <h3>Top Feature Importances</h3>
            <img src="plots/tuned_rf_feature_importance.png" alt="Random Forest Feature Importance">
        </div>
        
        <div class="model-section">
            <h2>Decision Tree Classification Model</h2>
            <h3>Best Parameters</h3>
            <table>
                <tr><th>Parameter</th><th>Value</th></tr>
    <tr><td>criterion</td><td>entropy</td></tr><tr><td>max_depth</td><td>22</td></tr><tr><td>max_features</td><td>None</td></tr><tr><td>min_samples_leaf</td><td>8</td></tr><tr><td>min_samples_split</td><td>16</td></tr><tr><td>splitter</td><td>best</td></tr>
            </table>
            
            <h3>Performance Metrics</h3>
            <ul>
                <li>Accuracy: <span class="metric">0.9870</span></li>
                <li>Tuning Time: <span class="metric">0.52 seconds</span></li>
            </ul>
            
            <h3>Classification Report</h3>
            <pre>              precision    recall  f1-score   support

           0       0.98      0.99      0.99      1933
           1       0.99      0.98      0.99      1842

    accuracy                           0.99      3775
   macro avg       0.99      0.99      0.99      3775
weighted avg       0.99      0.99      0.99      3775
</pre>
            
            <h3>Top Feature Importances</h3>
            <img src="plots/tuned_dt_feature_importance.png" alt="Decision Tree Feature Importance">
        </div>
        
        <div class="model-section">
            <h2>Naive Bayes Classification Model</h2>
            <h3>Best Parameters</h3>
            <table>
                <tr><th>Parameter</th><th>Value</th></tr>
    <tr><td>var_smoothing</td><td>2.0866527711063714e-06</td></tr>
            </table>
            
            <h3>Performance Metrics</h3>
            <ul>
                <li>Accuracy: <span class="metric">0.8585</span></li>
                <li>Tuning Time: <span class="metric">0.38 seconds</span></li>
            </ul>
            
            <h3>Classification Report</h3>
            <pre>              precision    recall  f1-score   support

           0       0.88      0.84      0.86      1933
           1       0.84      0.88      0.86      1842

    accuracy                           0.86      3775
   macro avg       0.86      0.86      0.86      3775
weighted avg       0.86      0.86      0.86      3775
</pre>
        </div>
        
        <h2>Model Comparison</h2>
        <table>
            <tr>
                <th>Model</th>
                <th>Primary Metric</th>
                <th>Tuning Time (seconds)</th>
            </tr>
            <tr>
                <td>Random Forest (Regression)</td>
                <td>R² Score: 0.9732</td>
                <td>62.20</td>
            </tr>
            <tr>
                <td>Decision Tree (Classification)</td>
                <td>Accuracy: 0.9870</td>
                <td>0.52</td>
            </tr>
            <tr>
                <td>Naive Bayes (Classification)</td>
                <td>Accuracy: 0.8585</td>
                <td>0.38</td>
            </tr>
        </table>
        
        <h2>Conclusion</h2>
        <p>
            After randomized hyperparameter tuning, the models show the following performance:
        </p>
        <ul>
            <li>The Random Forest regression model achieved an R² score of 0.9732, explaining 97.3% of the variance in the game sales data.</li>
            <li>The Decision Tree classification model achieved an accuracy of 0.9870 for classifying games into high or low sales categories.</li>
            <li>The Naive Bayes classification model achieved an accuracy of 0.8585, which is 1.5% better than the previous non-tuned version.</li>
        </ul>
        
        <h3>Randomized vs. Grid Search</h3>
        <p>
            This tuning was performed using RandomizedSearchCV instead of GridSearchCV, which has these advantages:
        </p>
        <ul>
            <li>Much faster execution time, allowing exploration of a wider parameter space</li>
            <li>Ability to search continuous distributions rather than discrete values</li>
            <li>Often finds comparable results to exhaustive grid search with significantly less computational cost</li>
        </ul>
        
        <h3>Recommendations</h3>
        <p>
            Based on the tuning results, we recommend:
        </p>
        <ul>
            <li>Using the tuned Random Forest model for sales prediction tasks with the optimal parameters found.</li>
            <li>Using the tuned Decision Tree model for classification tasks, as it outperforms the Naive Bayes model.</li>
            <li>Considering the computational cost vs. benefit when choosing between models for production deployment.</li>
        </ul>
    </body>
    </html>
    