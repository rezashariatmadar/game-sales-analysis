{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (18874, 14)\n",
      "\n",
      "Data Types:\n",
      "title            object\n",
      "console          object\n",
      "genre            object\n",
      "publisher        object\n",
      "developer        object\n",
      "critic_score    float64\n",
      "total_sales     float64\n",
      "na_sales        float64\n",
      "jp_sales        float64\n",
      "pal_sales       float64\n",
      "other_sales     float64\n",
      "release_date     object\n",
      "last_update      object\n",
      "release_year    float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "title               0\n",
      "console             0\n",
      "genre               0\n",
      "publisher           0\n",
      "developer           4\n",
      "critic_score        0\n",
      "total_sales         0\n",
      "na_sales         6252\n",
      "jp_sales        12176\n",
      "pal_sales        6067\n",
      "other_sales      3762\n",
      "release_date       90\n",
      "last_update     14871\n",
      "release_year       90\n",
      "dtype: int64\n",
      "\n",
      "Numeric columns: ['critic_score', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'release_year']\n",
      "\n",
      "Categorical columns: ['title', 'console', 'genre', 'publisher', 'developer', 'release_date', 'last_update']\n",
      "\n",
      "Variable Types:\n",
      "title: Nominal\n",
      "console: Nominal\n",
      "genre: Nominal\n",
      "publisher: Nominal\n",
      "developer: Nominal\n",
      "critic_score: Numeric (Continuous)\n",
      "total_sales: Numeric (Continuous)\n",
      "na_sales: Numeric (Continuous)\n",
      "jp_sales: Numeric (Continuous)\n",
      "pal_sales: Numeric (Continuous)\n",
      "other_sales: Numeric (Continuous)\n",
      "release_date: Nominal\n",
      "last_update: Nominal\n",
      "release_year: Numeric (Continuous)\n",
      "\n",
      "Statistics for critic_score:\n",
      "Mean: 7.413489456395041\n",
      "Median: 7.5\n",
      "Mode: 7.5\n",
      "Midrange: 5.5\n",
      "Five Number Summary:\n",
      "  Minimum: 1.0\n",
      "  Q1: 7.5\n",
      "  Median: 7.5\n",
      "  Q3: 7.5\n",
      "  Maximum: 10.0\n",
      "\n",
      "Statistics for total_sales:\n",
      "Mean: 0.3495729575076826\n",
      "Median: 0.12\n",
      "Mode: 0.01\n",
      "Midrange: 10.16\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 0.03\n",
      "  Median: 0.12\n",
      "  Q3: 0.35\n",
      "  Maximum: 20.32\n",
      "\n",
      "Statistics for na_sales:\n",
      "Mean: 0.2649722706385676\n",
      "Median: 0.12\n",
      "Mode: 0.04\n",
      "Midrange: 4.88\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 0.05\n",
      "  Median: 0.12\n",
      "  Q3: 0.28\n",
      "  Maximum: 9.76\n",
      "\n",
      "Statistics for jp_sales:\n",
      "Mean: 0.10187369363989252\n",
      "Median: 0.04\n",
      "Mode: 0.01\n",
      "Midrange: 1.065\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 0.02\n",
      "  Median: 0.04\n",
      "  Q3: 0.12\n",
      "  Maximum: 2.13\n",
      "\n",
      "Statistics for pal_sales:\n",
      "Mean: 0.14957445147185136\n",
      "Median: 0.04\n",
      "Mode: 0.0\n",
      "Midrange: 4.925\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 0.01\n",
      "  Median: 0.04\n",
      "  Q3: 0.14\n",
      "  Maximum: 9.85\n",
      "\n",
      "Statistics for other_sales:\n",
      "Mean: 0.043073716251985174\n",
      "Median: 0.01\n",
      "Mode: 0.0\n",
      "Midrange: 1.56\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 0.0\n",
      "  Median: 0.01\n",
      "  Q3: 0.03\n",
      "  Maximum: 3.12\n",
      "\n",
      "Statistics for release_year:\n",
      "Mean: 2007.6865949744463\n",
      "Median: 2008.0\n",
      "Mode: 2009.0\n",
      "Midrange: 1998.5\n",
      "Five Number Summary:\n",
      "  Minimum: 1977.0\n",
      "  Q1: 2004.0\n",
      "  Median: 2008.0\n",
      "  Q3: 2011.0\n",
      "  Maximum: 2020.0\n",
      "\n",
      "Top 10 Publishers by Total Sales:\n",
      "Activision: 722.77 million\n",
      "Electronic Arts: 644.13 million\n",
      "EA Sports: 485.66 million\n",
      "Ubisoft: 462.32 million\n",
      "THQ: 320.89 million\n",
      "Sony Computer Entertainment: 307.24 million\n",
      "Rockstar Games: 239.67 million\n",
      "Konami: 210.12 million\n",
      "Sega: 206.35 million\n",
      "Nintendo: 139.50 million\n",
      "\n",
      "Top 10 Best-Selling Games:\n",
      "1. Grand Theft Auto V (PS3): 20.32 million\n",
      "2. Grand Theft Auto V (PS4): 19.39 million\n",
      "3. Grand Theft Auto: Vice City (PS2): 16.15 million\n",
      "4. Grand Theft Auto V (X360): 15.86 million\n",
      "5. Call of Duty: Black Ops 3 (PS4): 15.09 million\n",
      "6. Call of Duty: Modern Warfare 3 (X360): 14.82 million\n",
      "7. Call of Duty: Black Ops (X360): 14.74 million\n",
      "8. Red Dead Redemption 2 (PS4): 13.94 million\n",
      "9. Call of Duty: Black Ops II (X360): 13.86 million\n",
      "10. Call of Duty: Black Ops II (PS3): 13.80 million\n",
      "1. Created histogram for total sales\n",
      "2. Created box plot for regional sales\n",
      "3. Created QQ Plot for critic scores\n",
      "4. Created correlation heatmap\n",
      "5. Created scatter plot between critic score and total sales\n",
      "6. Created bar chart for game genres\n",
      "7. Created pie chart for console distribution\n",
      "8. Created violin plot for total sales by genre\n",
      "9. Created quantile plot for total sales\n",
      "10. Created pair plot for regional sales and critic score\n",
      "11. Created line plot of game releases by year\n",
      "12. Created heatmap of genre popularity over time\n",
      "\n",
      "Analysis complete. All plots have been saved to the 'plots' directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Create output directory for plots\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('vgchartz_cleaned.csv')\n",
    "\n",
    "# Basic info and statistical summary\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Convert release_year to numeric if needed\n",
    "if df['release_year'].dtype == 'object':\n",
    "    df['release_year'] = pd.to_numeric(df['release_year'], errors='coerce')\n",
    "\n",
    "# Identify variable types\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric columns:\", numeric_cols)\n",
    "print(\"\\nCategorical columns:\", categorical_cols)\n",
    "\n",
    "# Function to identify data type\n",
    "def identify_variable_type(col):\n",
    "    if df[col].dtype == 'object':\n",
    "        unique_values = df[col].nunique()\n",
    "        if unique_values == 2:\n",
    "            return \"Binary\"\n",
    "        elif unique_values <= 10:\n",
    "            return \"Nominal\" if not all(df[col].dropna().astype(str).str.isnumeric()) else \"Ordinal\"\n",
    "        else:\n",
    "            return \"Nominal\"\n",
    "    else:  # numeric\n",
    "        unique_values = df[col].nunique()\n",
    "        if unique_values == 2:\n",
    "            return \"Binary\"\n",
    "        elif unique_values <= 10:\n",
    "            return \"Ordinal\"\n",
    "        else:\n",
    "            return \"Numeric (Continuous)\"\n",
    "\n",
    "# Identify variable types\n",
    "print(\"\\nVariable Types:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {identify_variable_type(col)}\")\n",
    "\n",
    "# Calculate statistics for numeric variables\n",
    "def calculate_statistics(df, column):\n",
    "    if column in df.columns:\n",
    "        data = df[column].dropna()\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(data):\n",
    "            # Calculate statistics\n",
    "            mean = data.mean()\n",
    "            median = data.median()\n",
    "            mode = data.mode()[0]\n",
    "            midrange = (data.max() + data.min()) / 2\n",
    "            q1 = data.quantile(0.25)\n",
    "            q3 = data.quantile(0.75)\n",
    "            min_val = data.min()\n",
    "            max_val = data.max()\n",
    "            \n",
    "            print(f\"\\nStatistics for {column}:\")\n",
    "            print(f\"Mean: {mean}\")\n",
    "            print(f\"Median: {median}\")\n",
    "            print(f\"Mode: {mode}\")\n",
    "            print(f\"Midrange: {midrange}\")\n",
    "            print(f\"Five Number Summary:\")\n",
    "            print(f\"  Minimum: {min_val}\")\n",
    "            print(f\"  Q1: {q1}\")\n",
    "            print(f\"  Median: {median}\")\n",
    "            print(f\"  Q3: {q3}\")\n",
    "            print(f\"  Maximum: {max_val}\")\n",
    "            \n",
    "            return {\n",
    "                'mean': mean,\n",
    "                'median': median,\n",
    "                'mode': mode,\n",
    "                'midrange': midrange,\n",
    "                'min': min_val,\n",
    "                'q1': q1,\n",
    "                'q3': q3,\n",
    "                'max': max_val\n",
    "            }\n",
    "        else:\n",
    "            print(f\"\\n{column} is not numeric, skipping statistics.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"\\n{column} not found in dataset.\")\n",
    "        return None\n",
    "\n",
    "# Create the 10 required plots with gaming dataset adaptations\n",
    "def create_plots(df):\n",
    "    # 1. Histogram for total_sales\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['total_sales'], kde=True)\n",
    "    plt.title('Histogram of Total Game Sales')\n",
    "    plt.xlabel('Total Sales (millions)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/1_histogram_total_sales.png')\n",
    "    plt.close()\n",
    "    print(\"1. Created histogram for total sales\")\n",
    "    \n",
    "    # 2. Box Plot for sales across regions\n",
    "    sales_cols = ['na_sales', 'jp_sales', 'pal_sales', 'other_sales']\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df[sales_cols])\n",
    "    plt.title('Box Plot of Sales by Region')\n",
    "    plt.ylabel('Sales (millions)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/2_boxplot_regional_sales.png')\n",
    "    plt.close()\n",
    "    print(\"2. Created box plot for regional sales\")\n",
    "    \n",
    "    # 3. QQ Plot for critic scores\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    stats.probplot(df['critic_score'].dropna(), dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot of Critic Scores')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/3_qqplot_critic_score.png')\n",
    "    plt.close()\n",
    "    print(\"3. Created QQ Plot for critic scores\")\n",
    "    \n",
    "    # 4. Correlation Heatmap for numeric columns\n",
    "    numeric_game_cols = ['critic_score', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'release_year']\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr = df[numeric_game_cols].corr()\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    sns.heatmap(corr, mask=mask, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap of Sales and Scores')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/4_correlation_heatmap.png')\n",
    "    plt.close()\n",
    "    print(\"4. Created correlation heatmap\")\n",
    "    \n",
    "    # 5. Scatter Plot between critic_score and total_sales\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='critic_score', y='total_sales', data=df)\n",
    "    plt.title('Scatter Plot: Critic Score vs Total Sales')\n",
    "    plt.xlabel('Critic Score')\n",
    "    plt.ylabel('Total Sales (millions)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/5_scatterplot_score_sales.png')\n",
    "    plt.close()\n",
    "    print(\"5. Created scatter plot between critic score and total sales\")\n",
    "    \n",
    "    # 6. Bar Chart for game genres\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    genre_counts = df['genre'].value_counts().sort_values(ascending=False)\n",
    "    sns.barplot(x=genre_counts.index, y=genre_counts.values)\n",
    "    plt.title('Number of Games by Genre')\n",
    "    plt.xlabel('Genre')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/6_barchart_genres.png')\n",
    "    plt.close()\n",
    "    print(\"6. Created bar chart for game genres\")\n",
    "    \n",
    "    # 7. Pie Chart for console distribution\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    df['console'].value_counts().plot.pie(autopct='%1.1f%%')\n",
    "    plt.title('Distribution of Games by Console')\n",
    "    plt.ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/7_piechart_consoles.png')\n",
    "    plt.close()\n",
    "    print(\"7. Created pie chart for console distribution\")\n",
    "    \n",
    "    # 8. Violin Plot for total sales by genre\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    # Limit to top 10 genres if there are many\n",
    "    top_genres = df['genre'].value_counts().nlargest(10).index\n",
    "    genre_data = df[df['genre'].isin(top_genres)]\n",
    "    sns.violinplot(x='genre', y='total_sales', data=genre_data)\n",
    "    plt.title('Total Sales Distribution by Genre')\n",
    "    plt.xlabel('Genre')\n",
    "    plt.ylabel('Total Sales (millions)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/8_violinplot_sales_by_genre.png')\n",
    "    plt.close()\n",
    "    print(\"8. Created violin plot for total sales by genre\")\n",
    "    \n",
    "    # 9. Quantile Plot (Empirical CDF) for total sales\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sorted_data = np.sort(df['total_sales'].dropna())\n",
    "    y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "    plt.plot(sorted_data, y, marker='.', linestyle='none')\n",
    "    plt.title('Quantile Plot (Empirical CDF) of Total Sales')\n",
    "    plt.xlabel('Total Sales (millions)')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/9_quantile_plot_total_sales.png')\n",
    "    plt.close()\n",
    "    print(\"9. Created quantile plot for total sales\")\n",
    "    \n",
    "    # 10. Pair Plot for sales across regions\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    sns.pairplot(df[['na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'critic_score']], height=2.5)\n",
    "    plt.suptitle('Pair Plot of Regional Sales and Critic Score', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/10_pairplot_sales.png')\n",
    "    plt.close()\n",
    "    print(\"10. Created pair plot for regional sales and critic score\")\n",
    "\n",
    "    # 11. Line plot of game releases by year\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    year_counts = df.groupby('release_year').size()\n",
    "    year_counts.plot(kind='line', marker='o')\n",
    "    plt.title('Number of Games Released by Year')\n",
    "    plt.xlabel('Release Year')\n",
    "    plt.ylabel('Number of Games')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/11_lineplot_releases_by_year.png')\n",
    "    plt.close()\n",
    "    print(\"11. Created line plot of game releases by year\")\n",
    "    \n",
    "    # 12. Heatmap of genre popularity over time\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    genre_year = df.groupby(['release_year', 'genre']).size().unstack().fillna(0)\n",
    "    # Limit to top 10 genres if needed\n",
    "    if len(genre_year.columns) > 10:\n",
    "        top_genres = df['genre'].value_counts().nlargest(10).index\n",
    "        genre_year = genre_year[genre_year.columns.intersection(top_genres)]\n",
    "    sns.heatmap(genre_year.T, cmap='YlGnBu', linewidths=0.5)\n",
    "    plt.title('Genre Popularity Over Years')\n",
    "    plt.xlabel('Release Year')\n",
    "    plt.ylabel('Genre')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/12_heatmap_genre_by_year.png')\n",
    "    plt.close()\n",
    "    print(\"12. Created heatmap of genre popularity over time\")\n",
    "\n",
    "# Calculate statistics for all numeric columns\n",
    "numeric_columns = ['critic_score', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'release_year']\n",
    "for col in numeric_columns:\n",
    "    calculate_statistics(df, col)\n",
    "\n",
    "# Calculate top publishers by total sales\n",
    "top_publishers = df.groupby('publisher')['total_sales'].sum().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 Publishers by Total Sales:\")\n",
    "for publisher, sales in top_publishers.items():\n",
    "    print(f\"{publisher}: {sales:.2f} million\")\n",
    "\n",
    "# Calculate top selling games\n",
    "top_games = df.sort_values('total_sales', ascending=False).head(10)[['title', 'console', 'total_sales']]\n",
    "print(\"\\nTop 10 Best-Selling Games:\")\n",
    "for i, (_, row) in enumerate(top_games.iterrows(), 1):\n",
    "    print(f\"{i}. {row['title']} ({row['console']}): {row['total_sales']:.2f} million\")\n",
    "\n",
    "# Generate the plots\n",
    "create_plots(df)\n",
    "\n",
    "print(\"\\nAnalysis complete. All plots have been saved to the 'plots' directory.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Original dataset shape: (18874, 14)\n",
      "\n",
      "===== DATA CLEANING =====\n",
      "Number of duplicate records: 0\n",
      "\n",
      "Missing Values:\n",
      "title               0\n",
      "console             0\n",
      "genre               0\n",
      "publisher           0\n",
      "developer           4\n",
      "critic_score        0\n",
      "total_sales         0\n",
      "na_sales         6252\n",
      "jp_sales        12176\n",
      "pal_sales        6067\n",
      "other_sales      3762\n",
      "release_date       90\n",
      "last_update     14871\n",
      "release_year       90\n",
      "dtype: int64\n",
      "After removing rows with missing essential data: (18874, 14)\n",
      "\n",
      "Checking for outliers and inconsistent values in numeric columns...\n",
      "critic_score: Range [1.0, 10.0]\n",
      "  Found 3956 potential outliers in critic_score\n",
      "  Outliers in critic_score capped to [7.50, 7.50]\n",
      "total_sales: Range [0.0, 20.32]\n",
      "  Found 1905 potential outliers in total_sales\n",
      "  Outliers in total_sales capped to [-0.45, 0.83]\n",
      "na_sales: Range [0.0, 9.76]\n",
      "  Found 1210 potential outliers in na_sales\n",
      "  Outliers in na_sales capped to [-0.30, 0.63]\n",
      "Skipping jp_sales due to high missing value rate\n",
      "pal_sales: Range [0.0, 9.85]\n",
      "  Found 1408 potential outliers in pal_sales\n",
      "  Outliers in pal_sales capped to [-0.18, 0.34]\n",
      "other_sales: Range [0.0, 3.12]\n",
      "  Found 1930 potential outliers in other_sales\n",
      "  Outliers in other_sales capped to [-0.04, 0.07]\n",
      "release_year: Range [1977.0, 2020.0]\n",
      "  Found 248 potential outliers in release_year\n",
      "  Outliers in release_year capped to [1993.50, 2021.50]\n",
      "\n",
      "Checking for logical inconsistencies...\n",
      "\n",
      "Filling remaining missing values...\n",
      "  Filled 6252 missing values in na_sales with median\n",
      "  Filled 12176 missing values in jp_sales with median\n",
      "  Filled 6067 missing values in pal_sales with median\n",
      "  Filled 3762 missing values in other_sales with median\n",
      "  Filled 90 missing values in release_year with median\n",
      "\n",
      "===== DATA TRANSFORMATION =====\n",
      "\n",
      "Standardizing numeric features...\n",
      "Standardized 7 numeric columns\n",
      "\n",
      "Encoding categorical variables...\n",
      "Processing console...\n",
      "  39 unique values\n",
      "  Applied frequency encoding to console\n",
      "Processing genre...\n",
      "  20 unique values\n",
      "  Applied frequency encoding to genre\n",
      "Processing publisher...\n",
      "  739 unique values\n",
      "  Applied frequency encoding to publisher\n",
      "Processing developer...\n",
      "  2866 unique values\n",
      "  Applied frequency encoding to developer\n",
      "\n",
      "Creating engineered features...\n",
      "Created na_sales_ratio (NA sales / total sales)\n",
      "Created jp_sales_ratio (Japan sales / total sales)\n",
      "Created pal_sales_ratio (PAL sales / total sales)\n",
      "Created game_age feature (years since release)\n",
      "Created sales_per_year feature\n",
      "Created critic_score category and dummy variables\n",
      "\n",
      "After removing remaining NaN values: (4000, 24)\n",
      "\n",
      "Processed data saved to processed_data/vgchartz_processed.csv\n",
      "Final dataset shape: (4000, 24)\n",
      "\n",
      "Summary of Transformations Applied:\n",
      "1. Removed duplicates\n",
      "2. Handled missing values in essential columns\n",
      "3. Capped outliers in numeric columns\n",
      "4. Fixed logical inconsistencies\n",
      "5. Standardized numeric features\n",
      "6. Encoded categorical variables\n",
      "7. Created engineered features\n",
      "\n",
      "First few rows of processed dataset:\n",
      "                       title  critic_score  total_sales  na_sales  jp_sales  \\\n",
      "1         Grand Theft Auto V           0.0     2.217843   2.80823  5.180602   \n",
      "4  Call of Duty: Black Ops 3           0.0     2.217843   2.80823  3.351165   \n",
      "7      Red Dead Redemption 2           0.0     2.217843   2.80823  1.425442   \n",
      "\n",
      "   pal_sales  other_sales release_date last_update  release_year  ...  \\\n",
      "1   2.740523       2.3061   2014-11-18  2018-01-03      1.087773  ...   \n",
      "4   2.740523       2.3061   2015-11-06  2018-01-14      1.262590  ...   \n",
      "7   2.740523       2.3061   2018-10-26  2018-11-02      1.787040  ...   \n",
      "\n",
      "   na_sales_ratio  jp_sales_ratio  pal_sales_ratio     game_age  \\\n",
      "1        1.266199        2.335874          1.23567  2021.912227   \n",
      "4        1.266199        1.511002          1.23567  2021.737410   \n",
      "7        1.266199        0.642715          1.23567  2021.212960   \n",
      "\n",
      "   sales_per_year  rating_Poor  rating_Average  rating_Good  rating_Great  \\\n",
      "1        0.001096        False           False        False         False   \n",
      "4        0.001096        False           False        False         False   \n",
      "7        0.001097        False           False        False         False   \n",
      "\n",
      "   rating_Excellent  \n",
      "1             False  \n",
      "4             False  \n",
      "7             False  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "\n",
      "Column list of processed dataset:\n",
      "['title', 'critic_score', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'release_date', 'last_update', 'release_year', 'console_freq', 'genre_freq', 'publisher_freq', 'developer_freq', 'na_sales_ratio', 'jp_sales_ratio', 'pal_sales_ratio', 'game_age', 'sales_per_year', 'rating_Poor', 'rating_Average', 'rating_Good', 'rating_Great', 'rating_Excellent']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import os\n",
    "\n",
    "# Create directory for processed data and visualizations\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "os.makedirs('cleaning_plots', exist_ok=True)\n",
    "\n",
    "print(\"Loading the dataset...\")\n",
    "# Load the dataset and create a copy for preprocessing\n",
    "df = pd.read_csv('vgchartz_cleaned.csv')\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# ============= DATA CLEANING =============\n",
    "print(\"\\n===== DATA CLEANING =====\")\n",
    "\n",
    "# 1. Check for duplicate records\n",
    "duplicates = df_processed.duplicated().sum()\n",
    "print(f\"Number of duplicate records: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    print(f\"After removing duplicates: {df_processed.shape}\")\n",
    "\n",
    "# 2. Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_values = df_processed.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Only keep rows where essential columns are not null\n",
    "essential_columns = ['title', 'console', 'genre', 'publisher', 'critic_score', 'total_sales']\n",
    "df_processed = df_processed.dropna(subset=essential_columns)\n",
    "print(f\"After removing rows with missing essential data: {df_processed.shape}\")\n",
    "\n",
    "# Convert release_year to numeric if needed\n",
    "if df_processed['release_year'].dtype == 'object':\n",
    "    df_processed['release_year'] = pd.to_numeric(df_processed['release_year'], errors='coerce')\n",
    "\n",
    "# 3. Check for inconsistent values in numeric columns\n",
    "numeric_cols = ['critic_score', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', \n",
    "                'other_sales', 'release_year']\n",
    "\n",
    "print(\"\\nChecking for outliers and inconsistent values in numeric columns...\")\n",
    "for col in numeric_cols:\n",
    "    if col in df_processed.columns:\n",
    "        # Skip columns with too many NaN values\n",
    "        if df_processed[col].isna().sum() > len(df_processed) * 0.5:\n",
    "            print(f\"Skipping {col} due to high missing value rate\")\n",
    "            continue\n",
    "            \n",
    "        # Print range and check for unusual values\n",
    "        min_val = df_processed[col].min()\n",
    "        max_val = df_processed[col].max()\n",
    "        print(f\"{col}: Range [{min_val}, {max_val}]\")\n",
    "        \n",
    "        # Identify potential outliers using IQR method\n",
    "        Q1 = df_processed[col].quantile(0.25)\n",
    "        Q3 = df_processed[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df_processed[(df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)]\n",
    "        \n",
    "        if not outliers.empty:\n",
    "            print(f\"  Found {len(outliers)} potential outliers in {col}\")\n",
    "            \n",
    "            # Visualize outliers\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.boxplot(x=df_processed[col])\n",
    "            plt.title(f'Boxplot of {col} - Outlier Detection')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'cleaning_plots/outliers_{col}.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # Cap outliers to the boundary values (alternative to dropping)\n",
    "            df_processed[col] = df_processed[col].clip(lower_bound, upper_bound)\n",
    "            print(f\"  Outliers in {col} capped to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "\n",
    "# 4. Check for logical inconsistencies\n",
    "print(\"\\nChecking for logical inconsistencies...\")\n",
    "\n",
    "# Critic score should be between 0 and 10\n",
    "if df_processed['critic_score'].max() > 10 or df_processed['critic_score'].min() < 0:\n",
    "    print(f\"Found critic_score values outside the expected range [0, 10]\")\n",
    "    df_processed['critic_score'] = df_processed['critic_score'].clip(0, 10)\n",
    "    print(\"  Capped critic_score to [0, 10]\")\n",
    "\n",
    "# Sales figures should be non-negative\n",
    "for col in ['total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales']:\n",
    "    if col in df_processed.columns and df_processed[col].min() < 0:\n",
    "        print(f\"Found negative values in {col}\")\n",
    "        df_processed[col] = df_processed[col].clip(0, None)\n",
    "        print(f\"  Capped {col} to non-negative values\")\n",
    "\n",
    "# Release year should be reasonable (e.g., 1970-2023)\n",
    "if 'release_year' in df_processed.columns:\n",
    "    invalid_years = df_processed[(df_processed['release_year'] < 1970) | (df_processed['release_year'] > 2023)]\n",
    "    if not invalid_years.empty:\n",
    "        print(f\"Found {len(invalid_years)} records with unusual release_year values\")\n",
    "        df_processed['release_year'] = df_processed['release_year'].clip(1970, 2023)\n",
    "        print(\"  Capped release_year to [1970, 2023]\")\n",
    "\n",
    "# 5. Fill remaining missing numeric values\n",
    "print(\"\\nFilling remaining missing values...\")\n",
    "for col in numeric_cols:\n",
    "    if col in df_processed.columns and df_processed[col].isna().any():\n",
    "        missing_count = df_processed[col].isna().sum()\n",
    "        df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "        print(f\"  Filled {missing_count} missing values in {col} with median\")\n",
    "\n",
    "# ============= DATA TRANSFORMATION =============\n",
    "print(\"\\n===== DATA TRANSFORMATION =====\")\n",
    "\n",
    "# 1. Standardize numeric features\n",
    "print(\"\\nStandardizing numeric features...\")\n",
    "numeric_cols_to_scale = [col for col in numeric_cols if col in df_processed.columns]\n",
    "scaler = StandardScaler()\n",
    "df_processed[numeric_cols_to_scale] = scaler.fit_transform(df_processed[numeric_cols_to_scale])\n",
    "print(f\"Standardized {len(numeric_cols_to_scale)} numeric columns\")\n",
    "\n",
    "# 2. Encoding categorical variables\n",
    "categorical_cols = ['console', 'genre', 'publisher', 'developer']\n",
    "print(\"\\nEncoding categorical variables...\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_processed.columns:\n",
    "        print(f\"Processing {col}...\")\n",
    "        # Check number of unique values\n",
    "        n_unique = df_processed[col].nunique()\n",
    "        print(f\"  {n_unique} unique values\")\n",
    "        \n",
    "        # For binary variables, use simple label encoding\n",
    "        if n_unique == 2:\n",
    "            le = LabelEncoder()\n",
    "            df_processed[col] = le.fit_transform(df_processed[col])\n",
    "            print(f\"  Applied label encoding to {col}\")\n",
    "            mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            print(f\"  Mapping: {mapping}\")\n",
    "        \n",
    "        # For variables with limited categories, use one-hot encoding\n",
    "        elif n_unique <= 15:\n",
    "            # Get dummies and drop the first to avoid multicollinearity\n",
    "            dummies = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)\n",
    "            df_processed = pd.concat([df_processed, dummies], axis=1)\n",
    "            df_processed.drop(col, axis=1, inplace=True)\n",
    "            print(f\"  Applied one-hot encoding to {col}, created {len(dummies.columns)} new features\")\n",
    "        \n",
    "        # For high-cardinality variables, consider frequency encoding\n",
    "        else:\n",
    "            # Create a frequency map\n",
    "            freq_map = df_processed[col].value_counts(normalize=True).to_dict()\n",
    "            df_processed[f'{col}_freq'] = df_processed[col].map(freq_map)\n",
    "            df_processed.drop(col, axis=1, inplace=True)\n",
    "            print(f\"  Applied frequency encoding to {col}\")\n",
    "\n",
    "# 3. Feature Engineering\n",
    "print(\"\\nCreating engineered features...\")\n",
    "\n",
    "# Calculate sales ratios between regions\n",
    "if all(col in df_processed.columns for col in ['na_sales', 'total_sales']):\n",
    "    df_processed['na_sales_ratio'] = df_processed['na_sales'] / df_processed['total_sales']\n",
    "    print(\"Created na_sales_ratio (NA sales / total sales)\")\n",
    "\n",
    "if all(col in df_processed.columns for col in ['jp_sales', 'total_sales']):\n",
    "    df_processed['jp_sales_ratio'] = df_processed['jp_sales'] / df_processed['total_sales']\n",
    "    print(\"Created jp_sales_ratio (Japan sales / total sales)\")\n",
    "\n",
    "if all(col in df_processed.columns for col in ['pal_sales', 'total_sales']):\n",
    "    df_processed['pal_sales_ratio'] = df_processed['pal_sales'] / df_processed['total_sales']\n",
    "    print(\"Created pal_sales_ratio (PAL sales / total sales)\")\n",
    "\n",
    "# Create age of game feature\n",
    "if 'release_year' in df_processed.columns:\n",
    "    current_year = 2023\n",
    "    df_processed['game_age'] = current_year - df_processed['release_year']\n",
    "    print(\"Created game_age feature (years since release)\")\n",
    "\n",
    "# Create sales per year feature\n",
    "if all(col in df_processed.columns for col in ['total_sales', 'game_age']):\n",
    "    df_processed['sales_per_year'] = df_processed['total_sales'] / (df_processed['game_age'] + 1)  # +1 to avoid division by zero\n",
    "    print(\"Created sales_per_year feature\")\n",
    "\n",
    "# Create critic score categorical variable\n",
    "if 'critic_score' in df_processed.columns:\n",
    "    bins = [0, 5, 7, 8, 9, 10]\n",
    "    labels = ['Poor', 'Average', 'Good', 'Great', 'Excellent']\n",
    "    df_processed['critic_score_category'] = pd.cut(df_processed['critic_score'], bins=bins, labels=labels)\n",
    "    dummies = pd.get_dummies(df_processed['critic_score_category'], prefix='rating')\n",
    "    df_processed = pd.concat([df_processed, dummies], axis=1)\n",
    "    df_processed.drop('critic_score_category', axis=1, inplace=True)\n",
    "    print(\"Created critic_score category and dummy variables\")\n",
    "\n",
    "# 4. Remove any remaining NaN values\n",
    "df_processed = df_processed.dropna()\n",
    "print(f\"\\nAfter removing remaining NaN values: {df_processed.shape}\")\n",
    "\n",
    "# 5. Save the processed data\n",
    "processed_file = 'processed_data/vgchartz_processed.csv'\n",
    "df_processed.to_csv(processed_file, index=False)\n",
    "print(f\"\\nProcessed data saved to {processed_file}\")\n",
    "print(f\"Final dataset shape: {df_processed.shape}\")\n",
    "\n",
    "# Summary of transformations\n",
    "print(\"\\nSummary of Transformations Applied:\")\n",
    "print(\"1. Removed duplicates\")\n",
    "print(\"2. Handled missing values in essential columns\")\n",
    "print(\"3. Capped outliers in numeric columns\")\n",
    "print(\"4. Fixed logical inconsistencies\")\n",
    "print(\"5. Standardized numeric features\")\n",
    "print(\"6. Encoded categorical variables\")\n",
    "print(\"7. Created engineered features\")\n",
    "\n",
    "# Print head of processed dataset\n",
    "print(\"\\nFirst few rows of processed dataset:\")\n",
    "print(df_processed.head(3))\n",
    "print(\"\\nColumn list of processed dataset:\")\n",
    "print(df_processed.columns.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed video game sales dataset...\n",
      "Dataset shape: (4000, 24)\n",
      "\n",
      "Checking for non-numeric columns...\n",
      "Found 16 numeric columns out of 24 total columns\n",
      "Selected 16 features for clustering\n",
      "Features: ['critic_score', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'release_year', 'console_freq', 'genre_freq', 'publisher_freq', 'developer_freq', 'na_sales_ratio', 'jp_sales_ratio', 'pal_sales_ratio', 'game_age', 'sales_per_year']\n",
      "\n",
      "Too many features, selecting most important ones...\n",
      "Using 12 key features\n",
      "Key features: ['critic_score', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'release_year', 'na_sales_ratio', 'jp_sales_ratio', 'pal_sales_ratio', 'game_age', 'sales_per_year']\n",
      "\n",
      "Finding optimal number of clusters using Elbow Method...\n",
      "For n_clusters = 2, silhouette score is 0.893\n",
      "For n_clusters = 3, silhouette score is 0.902\n",
      "For n_clusters = 4, silhouette score is 0.882\n",
      "For n_clusters = 5, silhouette score is 0.881\n",
      "For n_clusters = 6, silhouette score is 0.488\n",
      "For n_clusters = 7, silhouette score is 0.515\n",
      "For n_clusters = 8, silhouette score is 0.513\n",
      "For n_clusters = 9, silhouette score is 0.518\n",
      "For n_clusters = 10, silhouette score is 0.525\n",
      "\n",
      "Optimal number of clusters based on silhouette score: 3\n",
      "\n",
      "Performing K-means clustering with 3 clusters...\n",
      "\n",
      "Cluster distribution:\n",
      "Cluster 0: 3970 instances (99.25%)\n",
      "Cluster 1: 26 instances (0.65%)\n",
      "Cluster 2: 4 instances (0.10%)\n",
      "\n",
      "Analyzing clusters...\n",
      "\n",
      "Cluster centers (mean values for each feature):\n",
      "         critic_score  total_sales  na_sales  jp_sales  pal_sales  \\\n",
      "Cluster                                                             \n",
      "0                 0.0    -0.053273 -0.071551 -0.051204   0.131475   \n",
      "1                 0.0     0.012260 -0.446453 -0.000334   0.241332   \n",
      "2                 0.0     0.015132 -0.725392  0.919940  -0.582265   \n",
      "\n",
      "         other_sales  release_year  na_sales_ratio  jp_sales_ratio  \\\n",
      "Cluster                                                              \n",
      "0           0.108315      1.207217        0.843532        0.262870   \n",
      "1           0.377451      1.128116      -21.433360      -26.919195   \n",
      "2          -0.201144      1.262590      -47.936847       60.793397   \n",
      "\n",
      "         pal_sales_ratio     game_age  sales_per_year  \n",
      "Cluster                                                \n",
      "0               0.682969  2021.792783       -0.000026  \n",
      "1              18.106272  2021.871884        0.000006  \n",
      "2             -38.478467  2021.737410        0.000007  \n",
      "\n",
      "Visualizing clusters using PCA...\n",
      "\n",
      "Analyzing relationship between clusters and total sales...\n",
      "\n",
      "Average sales by cluster:\n",
      "Cluster\n",
      "0   -0.053273\n",
      "1    0.012260\n",
      "2    0.015132\n",
      "Name: total_sales, dtype: float64\n",
      "\n",
      "Average critic scores by cluster:\n",
      "Cluster\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "Name: critic_score, dtype: float64\n",
      "\n",
      "Selected 10 key features for detailed analysis\n",
      "Key features: ['jp_sales_ratio', 'pal_sales_ratio', 'na_sales_ratio', 'jp_sales', 'pal_sales', 'na_sales', 'other_sales', 'game_age', 'release_year', 'total_sales']\n",
      "\n",
      "Analyzing regional sales patterns by cluster...\n",
      "\n",
      "Cluster Characteristics Summary:\n",
      "\n",
      "Cluster 0:\n",
      "  - total_sales: low (-0.05, z-score: -1.41)\n",
      "  - na_sales: high (-0.07, z-score: 1.28)\n",
      "  - na_sales_ratio: high (0.84, z-score: 1.19)\n",
      "  - jp_sales: low (-0.05, z-score: -0.76)\n",
      "  - pal_sales: high (0.13, z-score: 0.55)\n",
      "  - Dominant sales region: na_sales_ratio\n",
      "  - Size: 3970 games (99.25% of dataset)\n",
      "\n",
      "Cluster 1:\n",
      "  - game_age: high (2021.87, z-score: 1.29)\n",
      "  - release_year: low (1.13, z-score: -1.29)\n",
      "  - other_sales: high (0.38, z-score: 1.20)\n",
      "  - jp_sales_ratio: low (-26.92, z-score: -1.04)\n",
      "  - pal_sales_ratio: high (18.11, z-score: 1.04)\n",
      "  - Dominant sales region: pal_sales_ratio\n",
      "  - Size: 26 games (0.65% of dataset)\n",
      "\n",
      "Cluster 2:\n",
      "  - jp_sales: high (0.92, z-score: 1.41)\n",
      "  - pal_sales: low (-0.58, z-score: -1.40)\n",
      "  - pal_sales_ratio: low (-38.48, z-score: -1.35)\n",
      "  - jp_sales_ratio: high (60.79, z-score: 1.35)\n",
      "  - na_sales_ratio: low (-47.94, z-score: -1.26)\n",
      "  - Dominant sales region: jp_sales_ratio\n",
      "  - Size: 4 games (0.10% of dataset)\n",
      "\n",
      "Sample titles from each cluster:\n",
      "\n",
      "Cluster 0 sample titles:\n",
      "  - Penguins of Madagascar\n",
      "  - RC Revenge Pro\n",
      "  - Persona 4: Dancing All Night\n",
      "  - IDOLiSH7 Twelve Fantasia!\n",
      "  - State of Decay 2\n",
      "\n",
      "Cluster 1 sample titles:\n",
      "  - Lumines: Electronic Symphony\n",
      "  - Harvest Moon: The Land of Origin\n",
      "  - We Sing Pop!\n",
      "  - Ride\n",
      "  - Fuel\n",
      "\n",
      "Cluster 2 sample titles:\n",
      "  - Toukiden: The Age of Demons\n",
      "  - Persona 5\n",
      "  - 7th Dragon III Code: VFD\n",
      "  - Atelier Meruru: Alchemist of Arland 3\n",
      "\n",
      "K-means clustering analysis complete. Results saved to 'clustering_results' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Create directory for clustering results\n",
    "os.makedirs('clustering_results', exist_ok=True)\n",
    "\n",
    "# Load the preprocessed data\n",
    "print(\"Loading preprocessed video game sales dataset...\")\n",
    "df = pd.read_csv('processed_data/vgchartz_processed.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Ensure we only have numeric data for clustering\n",
    "print(\"\\nChecking for non-numeric columns...\")\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "numeric_columns = numeric_df.columns.tolist()\n",
    "print(f\"Found {len(numeric_columns)} numeric columns out of {len(df.columns)} total columns\")\n",
    "\n",
    "# Also exclude any identifier columns if present\n",
    "features = [col for col in numeric_columns if 'id' not in col.lower()]\n",
    "print(f\"Selected {len(features)} features for clustering\")\n",
    "print(\"Features:\", features)\n",
    "\n",
    "# Use a subset of features if there are too many\n",
    "if len(features) > 15:\n",
    "    print(\"\\nToo many features, selecting most important ones...\")\n",
    "    # Prioritize key metrics for clustering\n",
    "    key_features = [col for col in features if any(term in col.lower() for term in \n",
    "                   ['sales', 'score', 'year', 'age', 'ratio'])]\n",
    "    if len(key_features) >= 5:  # Ensure we have a reasonable number of features\n",
    "        features = key_features\n",
    "        print(f\"Using {len(features)} key features\")\n",
    "        print(\"Key features:\", features)\n",
    "\n",
    "# Verify there are no NaN values in the selected features\n",
    "X_df = df[features].copy()\n",
    "nan_counts = X_df.isna().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(\"\\nWarning: Found NaN values in features. Filling with feature means.\")\n",
    "    X_df = X_df.fillna(X_df.mean())\n",
    "\n",
    "# Check for string values that might have been encoded as objects\n",
    "for col in X_df.columns:\n",
    "    if X_df[col].dtype == 'object':\n",
    "        print(f\"Converting column {col} to numeric, errors will be set to NaN\")\n",
    "        X_df[col] = pd.to_numeric(X_df[col], errors='coerce')\n",
    "        X_df[col] = X_df[col].fillna(X_df[col].mean())\n",
    "\n",
    "X = X_df.values\n",
    "\n",
    "# Determine optimal number of clusters using Elbow Method\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "max_clusters = 10\n",
    "\n",
    "print(\"\\nFinding optimal number of clusters using Elbow Method...\")\n",
    "for k in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score (only if k > 1)\n",
    "    if k > 1:\n",
    "        labels = kmeans.labels_\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        print(f\"For n_clusters = {k}, silhouette score is {silhouette_avg:.3f}\")\n",
    "\n",
    "# Plot Elbow Method results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(2, max_clusters + 1), inertia, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_results/kmeans_elbow_method.png')\n",
    "plt.close()\n",
    "\n",
    "# Based on the elbow method and silhouette scores, choose the optimal number of clusters\n",
    "# This is a simple heuristic - in practice, you would analyze the plots and choose accordingly\n",
    "optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because we started from k=2\n",
    "print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_k}\")\n",
    "\n",
    "# Perform K-means clustering with the optimal number of clusters\n",
    "print(f\"\\nPerforming K-means clustering with {optimal_k} clusters...\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df_cluster = df.copy()\n",
    "df_cluster['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Count instances in each cluster\n",
    "cluster_counts = df_cluster['Cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster distribution:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count} instances ({count/len(df_cluster)*100:.2f}%)\")\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\nAnalyzing clusters...\")\n",
    "cluster_analysis = df_cluster.groupby('Cluster')[features].mean()\n",
    "print(\"\\nCluster centers (mean values for each feature):\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Save cluster analysis to CSV\n",
    "cluster_analysis.to_csv('clustering_results/kmeans_cluster_analysis.csv')\n",
    "\n",
    "# Visualize clusters using PCA for dimensionality reduction\n",
    "print(\"\\nVisualizing clusters using PCA...\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_cluster['Cluster'], cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'Video Game Clusters Visualization with PCA (K-means, k={optimal_k})')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('clustering_results/kmeans_pca_visualization.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze relationship between clusters and total sales\n",
    "print(\"\\nAnalyzing relationship between clusters and total sales...\")\n",
    "if 'total_sales' in features:\n",
    "    sales_by_cluster = df_cluster.groupby('Cluster')['total_sales'].mean().sort_values()\n",
    "    print(\"\\nAverage sales by cluster:\")\n",
    "    print(sales_by_cluster)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sales_by_cluster.plot(kind='bar')\n",
    "    plt.title('Average Total Sales by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Average Sales')\n",
    "    plt.axhline(y=df_cluster['total_sales'].mean(), color='red', linestyle='--', label='Overall Average')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('clustering_results/kmeans_sales_by_cluster.png')\n",
    "    plt.close()\n",
    "\n",
    "# Analyze relationship between clusters and critic scores if available\n",
    "if 'critic_score' in features:\n",
    "    scores_by_cluster = df_cluster.groupby('Cluster')['critic_score'].mean().sort_values()\n",
    "    print(\"\\nAverage critic scores by cluster:\")\n",
    "    print(scores_by_cluster)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scores_by_cluster.plot(kind='bar')\n",
    "    plt.title('Average Critic Score by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Average Critic Score')\n",
    "    plt.axhline(y=df_cluster['critic_score'].mean(), color='red', linestyle='--', label='Overall Average')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('clustering_results/kmeans_scores_by_cluster.png')\n",
    "    plt.close()\n",
    "\n",
    "# Analyze key features by cluster\n",
    "# Select a subset of important features to visualize\n",
    "if len(features) > 10:\n",
    "    # Get the most variable features across clusters\n",
    "    feature_variance = cluster_analysis.var().sort_values(ascending=False)\n",
    "    key_features = feature_variance.head(10).index.tolist()\n",
    "else:\n",
    "    key_features = features\n",
    "\n",
    "print(f\"\\nSelected {len(key_features)} key features for detailed analysis\")\n",
    "print(\"Key features:\", key_features)\n",
    "\n",
    "# Create a heatmap of cluster centers for key features\n",
    "plt.figure(figsize=(14, 8))\n",
    "key_cluster_centers = cluster_analysis[key_features]\n",
    "# Normalize the data for better visualization\n",
    "scaler = StandardScaler()\n",
    "key_cluster_centers_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(key_cluster_centers),\n",
    "    index=key_cluster_centers.index,\n",
    "    columns=key_cluster_centers.columns\n",
    ")\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(key_cluster_centers_scaled, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Normalized Cluster Centers for Key Features (Video Games)')\n",
    "plt.ylabel('Cluster')\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_results/kmeans_key_features_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "# Regional sales comparison by cluster if those columns exist\n",
    "regional_cols = [col for col in features if any(region in col for region in ['na_', 'jp_', 'pal_', 'other_']) and 'sales' in col]\n",
    "if regional_cols:\n",
    "    print(\"\\nAnalyzing regional sales patterns by cluster...\")\n",
    "    regional_means = df_cluster.groupby('Cluster')[regional_cols].mean()\n",
    "    \n",
    "    # Create a radar chart for regional sales comparison\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Set plot parameters\n",
    "    categories = regional_cols\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Create angle for each feature\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Initialize the plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Add each cluster\n",
    "    for i in range(optimal_k):\n",
    "        values = regional_means.iloc[i].values.tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        # Plot values\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=f'Cluster {i}')\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Fix axis to go in the right order and start at 12 o'clock\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    \n",
    "    # Draw axis lines for each angle and label\n",
    "    plt.xticks(angles[:-1], categories)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title('Regional Sales Patterns by Cluster', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('clustering_results/kmeans_regional_sales_radar.png')\n",
    "    plt.close()\n",
    "\n",
    "# Create a parallel coordinates plot for cluster visualization\n",
    "plt.figure(figsize=(15, 8))\n",
    "# Get a subset of data for parallel coordinates plot (can be too dense with all data)\n",
    "sample_size = min(1000, len(df_cluster))\n",
    "sample_indices = np.random.choice(len(df_cluster), sample_size, replace=False)\n",
    "sample_df = df_cluster.iloc[sample_indices].copy()\n",
    "\n",
    "# Standardize the data for parallel coordinates plot\n",
    "features_for_parallel = key_features[:7] if len(key_features) > 7 else key_features  # Limit to 7 features for readability\n",
    "scaler = StandardScaler()\n",
    "sample_df_scaled = sample_df.copy()\n",
    "sample_df_scaled[features_for_parallel] = scaler.fit_transform(sample_df[features_for_parallel])\n",
    "\n",
    "# Create parallel coordinates plot\n",
    "pd.plotting.parallel_coordinates(\n",
    "    sample_df_scaled, 'Cluster', \n",
    "    cols=features_for_parallel,\n",
    "    color=plt.cm.viridis(np.linspace(0, 1, optimal_k))\n",
    ")\n",
    "plt.title('Parallel Coordinates Plot of Video Game Clusters')\n",
    "plt.grid(False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_results/kmeans_parallel_coordinates.png')\n",
    "plt.close()\n",
    "\n",
    "# Summarize the characteristics of each cluster\n",
    "print(\"\\nCluster Characteristics Summary:\")\n",
    "for cluster in range(optimal_k):\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    # Get the top 5 distinctive features for this cluster (highest absolute z-scores)\n",
    "    cluster_features = key_cluster_centers_scaled.loc[cluster].abs().sort_values(ascending=False)\n",
    "    top_features = cluster_features.head(5).index.tolist()\n",
    "    \n",
    "    for feature in top_features:\n",
    "        raw_value = key_cluster_centers.loc[cluster, feature]\n",
    "        scaled_value = key_cluster_centers_scaled.loc[cluster, feature]\n",
    "        direction = \"high\" if scaled_value > 0 else \"low\"\n",
    "        print(f\"  - {feature}: {direction} ({raw_value:.2f}, z-score: {scaled_value:.2f})\")\n",
    "    \n",
    "    # Optional: Display additional cluster characteristics based on available metrics\n",
    "    if regional_cols:\n",
    "        dominant_region = regional_means.loc[cluster].idxmax()\n",
    "        print(f\"  - Dominant sales region: {dominant_region}\")\n",
    "        \n",
    "    # Add any other relevant information about the clusters\n",
    "    cluster_size = cluster_counts[cluster]\n",
    "    cluster_percentage = cluster_size / len(df_cluster) * 100\n",
    "    print(f\"  - Size: {cluster_size} games ({cluster_percentage:.2f}% of dataset)\")\n",
    "\n",
    "# Sample titles from each cluster\n",
    "print(\"\\nSample titles from each cluster:\")\n",
    "for cluster in range(optimal_k):\n",
    "    if 'title' in df.columns:\n",
    "        sample_titles = df_cluster[df_cluster['Cluster'] == cluster]['title'].sample(min(5, cluster_counts[cluster])).tolist()\n",
    "        print(f\"\\nCluster {cluster} sample titles:\")\n",
    "        for title in sample_titles:\n",
    "            print(f\"  - {title}\")\n",
    "\n",
    "print(\"\\nK-means clustering analysis complete. Results saved to 'clustering_results' directory.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed video game sales dataset...\n",
      "Dataset shape: (4000, 24)\n",
      "\n",
      "Checking for non-numeric columns...\n",
      "Found 16 numeric columns out of 24 total columns\n",
      "Selected 16 features for clustering\n",
      "Features: ['critic_score', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'release_year', 'console_freq', 'genre_freq', 'publisher_freq', 'developer_freq', 'na_sales_ratio', 'jp_sales_ratio', 'pal_sales_ratio', 'game_age', 'sales_per_year']\n",
      "\n",
      "Too many features, selecting most important ones...\n",
      "Using 12 key features\n",
      "Key features: ['critic_score', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'release_year', 'na_sales_ratio', 'jp_sales_ratio', 'pal_sales_ratio', 'game_age', 'sales_per_year']\n",
      "\n",
      "Standardizing features...\n",
      "\n",
      "Dataset too large for hierarchical clustering. Sampling 1000 records...\n",
      "Working with sampled dataset of shape: (1000, 12)\n",
      "Computing linkage matrix with Ward's method...\n",
      "\n",
      "Evaluating different numbers of clusters...\n",
      "For n_clusters = 2, silhouette score is 0.461\n",
      "For n_clusters = 3, silhouette score is 0.469\n",
      "For n_clusters = 4, silhouette score is 0.252\n",
      "For n_clusters = 5, silhouette score is 0.242\n",
      "For n_clusters = 6, silhouette score is 0.267\n",
      "For n_clusters = 7, silhouette score is 0.279\n",
      "For n_clusters = 8, silhouette score is 0.282\n",
      "For n_clusters = 9, silhouette score is 0.305\n",
      "For n_clusters = 10, silhouette score is 0.306\n",
      "\n",
      "Optimal number of clusters based on silhouette score: 3\n",
      "\n",
      "Performing hierarchical clustering with 3 clusters...\n",
      "\n",
      "Cluster distribution:\n",
      "Cluster 0: 243 instances (24.30%)\n",
      "Cluster 1: 2 instances (0.20%)\n",
      "Cluster 2: 755 instances (75.50%)\n",
      "\n",
      "Analyzing clusters...\n",
      "\n",
      "Cluster centers (mean values for each feature):\n",
      "         critic_score  total_sales  na_sales  jp_sales  pal_sales  \\\n",
      "Cluster                                                             \n",
      "0                 0.0     1.416466  1.009129  0.302104   1.492062   \n",
      "1                 0.0     0.015132 -0.679100  0.944011  -0.687750   \n",
      "2                 0.0    -0.578208 -0.483672 -0.154671  -0.352922   \n",
      "\n",
      "         other_sales  release_year  na_sales_ratio  jp_sales_ratio  \\\n",
      "Cluster                                                              \n",
      "0           1.573531      1.151081        0.411655       -0.025165   \n",
      "1          -0.201144      1.175182      -44.877682       62.384143   \n",
      "2          -0.425025      1.260043        0.788523        0.010632   \n",
      "\n",
      "         pal_sales_ratio     game_age  sales_per_year  \n",
      "Cluster                                                \n",
      "0               0.999364  2021.848919        0.000700  \n",
      "1             -45.449370  2021.824818        0.000007  \n",
      "2               0.682770  2021.739957       -0.000286  \n",
      "\n",
      "Visualizing clusters using PCA...\n",
      "\n",
      "Analyzing relationship between clusters and sales...\n",
      "\n",
      "Average total sales by cluster:\n",
      "Cluster\n",
      "2   -0.578208\n",
      "1    0.015132\n",
      "0    1.416466\n",
      "Name: total_sales, dtype: float64\n",
      "\n",
      "Average critic scores by cluster:\n",
      "Cluster\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "Name: critic_score, dtype: float64\n",
      "\n",
      "Selected 10 key features for detailed analysis\n",
      "Key features: ['jp_sales_ratio', 'pal_sales_ratio', 'na_sales_ratio', 'pal_sales', 'other_sales', 'total_sales', 'na_sales', 'jp_sales', 'release_year', 'game_age']\n",
      "\n",
      "Analyzing regional sales patterns by cluster...\n",
      "\n",
      "Cluster Characteristics Summary:\n",
      "\n",
      "Cluster 0:\n",
      "  - other_sales: high (1.57, z-score: 1.41)\n",
      "  - na_sales: high (1.01, z-score: 1.41)\n",
      "  - pal_sales: high (1.49, z-score: 1.40)\n",
      "  - total_sales: high (1.42, z-score: 1.35)\n",
      "  - release_year: low (1.15, z-score: -0.95)\n",
      "  - Dominant sales region: other_sales\n",
      "  - Size: 243 games (24.30% of dataset)\n",
      "\n",
      "Cluster 1:\n",
      "  - jp_sales_ratio: high (62.38, z-score: 1.41)\n",
      "  - pal_sales_ratio: low (-45.45, z-score: -1.41)\n",
      "  - na_sales_ratio: low (-44.88, z-score: -1.41)\n",
      "  - jp_sales: high (0.94, z-score: 1.29)\n",
      "  - pal_sales: low (-0.69, z-score: -0.87)\n",
      "  - Dominant sales region: jp_sales_ratio\n",
      "  - Size: 2 games (0.20% of dataset)\n",
      "\n",
      "Cluster 2:\n",
      "  - game_age: low (2021.74, z-score: -1.38)\n",
      "  - release_year: high (1.26, z-score: 1.38)\n",
      "  - jp_sales: low (-0.15, z-score: -1.15)\n",
      "  - total_sales: low (-0.58, z-score: -1.03)\n",
      "  - other_sales: low (-0.43, z-score: -0.83)\n",
      "  - Dominant sales region: na_sales_ratio\n",
      "  - Size: 755 games (75.50% of dataset)\n",
      "\n",
      "Sample titles from each cluster:\n",
      "\n",
      "Cluster 0 sample titles:\n",
      "  - Just Dance 4\n",
      "  - Call of Duty: Infinite Warfare\n",
      "  - The Walking Dead: A Telltale Games Series\n",
      "  - Angry Birds Trilogy\n",
      "  - NBA 2K16\n",
      "\n",
      "Cluster 1 sample titles:\n",
      "  - Atelier Meruru: Alchemist of Arland 3\n",
      "  - Persona 5\n",
      "\n",
      "Cluster 2 sample titles:\n",
      "  - Angry Birds: Star Wars\n",
      "  - Minecraft: Story Mode - Season Two\n",
      "  - We Happy Few\n",
      "  - MXGP Pro\n",
      "  - WildStar\n",
      "\n",
      "Creating correlation matrix between features and PCA components...\n",
      "\n",
      "Hierarchical clustering analysis complete. Results saved to 'hierarchical_results' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import os\n",
    "\n",
    "# Create directory for hierarchical clustering results\n",
    "os.makedirs('hierarchical_results', exist_ok=True)\n",
    "\n",
    "# Load the preprocessed data\n",
    "print(\"Loading preprocessed video game sales dataset...\")\n",
    "df = pd.read_csv('processed_data/vgchartz_processed.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Ensure we only have numeric data for clustering\n",
    "print(\"\\nChecking for non-numeric columns...\")\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "numeric_columns = numeric_df.columns.tolist()\n",
    "print(f\"Found {len(numeric_columns)} numeric columns out of {len(df.columns)} total columns\")\n",
    "\n",
    "# Also exclude any identifier columns if present\n",
    "features = [col for col in numeric_columns if 'id' not in col.lower()]\n",
    "print(f\"Selected {len(features)} features for clustering\")\n",
    "print(\"Features:\", features)\n",
    "\n",
    "# Use a subset of features if there are too many\n",
    "if len(features) > 15:\n",
    "    print(\"\\nToo many features, selecting most important ones...\")\n",
    "    # Prioritize key metrics for clustering\n",
    "    key_features = [col for col in features if any(term in col.lower() for term in \n",
    "                   ['sales', 'score', 'year', 'age', 'ratio'])]\n",
    "    if len(key_features) >= 5:  # Ensure we have a reasonable number of features\n",
    "        features = key_features\n",
    "        print(f\"Using {len(features)} key features\")\n",
    "        print(\"Key features:\", features)\n",
    "\n",
    "# Verify there are no NaN values in the selected features\n",
    "X_df = df[features].copy()\n",
    "nan_counts = X_df.isna().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(\"\\nWarning: Found NaN values in features. Filling with feature means.\")\n",
    "    X_df = X_df.fillna(X_df.mean())\n",
    "\n",
    "# Check for string values that might have been encoded as objects\n",
    "for col in X_df.columns:\n",
    "    if X_df[col].dtype == 'object':\n",
    "        print(f\"Converting column {col} to numeric, errors will be set to NaN\")\n",
    "        X_df[col] = pd.to_numeric(X_df[col], errors='coerce')\n",
    "        X_df[col] = X_df[col].fillna(X_df[col].mean())\n",
    "\n",
    "# Standardize the data for hierarchical clustering\n",
    "print(\"\\nStandardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "# Sample data if too large (hierarchical clustering can be memory intensive)\n",
    "max_samples = 1000  # Adjust based on your system's capabilities\n",
    "if len(X_scaled) > max_samples:\n",
    "    print(f\"\\nDataset too large for hierarchical clustering. Sampling {max_samples} records...\")\n",
    "    sample_indices = np.random.choice(len(X_scaled), max_samples, replace=False)\n",
    "    X_scaled_sample = X_scaled[sample_indices]\n",
    "    df_sample = df.iloc[sample_indices].copy()\n",
    "    print(f\"Working with sampled dataset of shape: {X_scaled_sample.shape}\")\n",
    "else:\n",
    "    X_scaled_sample = X_scaled\n",
    "    df_sample = df.copy()\n",
    "\n",
    "# Compute the linkage matrix using Ward's method\n",
    "print(\"Computing linkage matrix with Ward's method...\")\n",
    "Z = linkage(X_scaled_sample, method='ward')\n",
    "\n",
    "# Plot the dendrogram to visualize hierarchical structure\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Hierarchical Clustering Dendrogram for Video Game Sales')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=12,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.savefig('hierarchical_results/hierarchical_dendrogram.png')\n",
    "plt.close()\n",
    "\n",
    "# Evaluate different numbers of clusters using silhouette score\n",
    "print(\"\\nEvaluating different numbers of clusters...\")\n",
    "silhouette_scores = []\n",
    "max_clusters = 10\n",
    "\n",
    "for k in range(2, max_clusters + 1):\n",
    "    # Get cluster labels\n",
    "    labels = fcluster(Z, k, criterion='maxclust')\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(X_scaled_sample, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {k}, silhouette score is {silhouette_avg:.3f}\")\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score Method for Hierarchical Clustering')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.savefig('hierarchical_results/hierarchical_silhouette_scores.png')\n",
    "plt.close()\n",
    "\n",
    "# Choose optimal number of clusters based on silhouette scores\n",
    "optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because we started from k=2\n",
    "print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_k}\")\n",
    "\n",
    "# Apply hierarchical clustering with the optimal number of clusters\n",
    "print(f\"\\nPerforming hierarchical clustering with {optimal_k} clusters...\")\n",
    "labels = fcluster(Z, optimal_k, criterion='maxclust')\n",
    "df_sample['Cluster'] = labels - 1  # Convert to 0-indexed clusters\n",
    "\n",
    "# Count instances in each cluster\n",
    "cluster_counts = df_sample['Cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster distribution:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count} instances ({count/len(df_sample)*100:.2f}%)\")\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\nAnalyzing clusters...\")\n",
    "cluster_analysis = df_sample.groupby('Cluster')[features].mean()\n",
    "print(\"\\nCluster centers (mean values for each feature):\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Save cluster analysis to CSV\n",
    "cluster_analysis.to_csv('hierarchical_results/hierarchical_cluster_analysis.csv')\n",
    "\n",
    "# Visualize clusters using PCA for dimensionality reduction\n",
    "print(\"\\nVisualizing clusters using PCA...\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled_sample)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_sample['Cluster'], cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'Video Game Sales Clusters Visualization with PCA (Hierarchical, k={optimal_k})')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('hierarchical_results/hierarchical_pca_visualization.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze relationship between clusters and sales\n",
    "print(\"\\nAnalyzing relationship between clusters and sales...\")\n",
    "if 'total_sales' in features:\n",
    "    sales_by_cluster = df_sample.groupby('Cluster')['total_sales'].mean().sort_values()\n",
    "    print(\"\\nAverage total sales by cluster:\")\n",
    "    print(sales_by_cluster)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sales_by_cluster.plot(kind='bar')\n",
    "    plt.title('Average Total Sales by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Average Sales')\n",
    "    plt.axhline(y=df_sample['total_sales'].mean(), color='red', linestyle='--', label='Overall Average')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('hierarchical_results/hierarchical_sales_by_cluster.png')\n",
    "    plt.close()\n",
    "\n",
    "# Analyze relationship between clusters and critic scores if available\n",
    "if 'critic_score' in features:\n",
    "    scores_by_cluster = df_sample.groupby('Cluster')['critic_score'].mean().sort_values()\n",
    "    print(\"\\nAverage critic scores by cluster:\")\n",
    "    print(scores_by_cluster)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scores_by_cluster.plot(kind='bar')\n",
    "    plt.title('Average Critic Score by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Average Critic Score')\n",
    "    plt.axhline(y=df_sample['critic_score'].mean(), color='red', linestyle='--', label='Overall Average')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('hierarchical_results/hierarchical_scores_by_cluster.png')\n",
    "    plt.close()\n",
    "\n",
    "# Analyze key features by cluster\n",
    "# Select a subset of important features to visualize\n",
    "if len(features) > 10:\n",
    "    # Get the most variable features across clusters\n",
    "    feature_variance = cluster_analysis.var().sort_values(ascending=False)\n",
    "    key_features = feature_variance.head(10).index.tolist()\n",
    "else:\n",
    "    key_features = features\n",
    "\n",
    "print(f\"\\nSelected {len(key_features)} key features for detailed analysis\")\n",
    "print(\"Key features:\", key_features)\n",
    "\n",
    "# Create a heatmap of cluster centers for key features\n",
    "plt.figure(figsize=(14, 8))\n",
    "key_cluster_centers = cluster_analysis[key_features]\n",
    "# Normalize the data for better visualization\n",
    "key_cluster_centers_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(key_cluster_centers),\n",
    "    index=key_cluster_centers.index,\n",
    "    columns=key_cluster_centers.columns\n",
    ")\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(key_cluster_centers_scaled, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Normalized Cluster Centers for Key Features (Video Games)')\n",
    "plt.ylabel('Cluster')\n",
    "plt.tight_layout()\n",
    "plt.savefig('hierarchical_results/hierarchical_key_features_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "# Regional sales comparison by cluster if those columns exist\n",
    "regional_cols = [col for col in features if any(region in col for region in ['na_', 'jp_', 'pal_', 'other_']) and 'sales' in col]\n",
    "if regional_cols:\n",
    "    print(\"\\nAnalyzing regional sales patterns by cluster...\")\n",
    "    regional_means = df_sample.groupby('Cluster')[regional_cols].mean()\n",
    "    \n",
    "    # Create a radar chart for regional sales comparison\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Set plot parameters\n",
    "    categories = regional_cols\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Create angle for each feature\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Initialize the plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Add each cluster\n",
    "    for i in range(optimal_k):\n",
    "        if i in regional_means.index:  # Check if the cluster exists\n",
    "            values = regional_means.loc[i].values.tolist()\n",
    "            values += values[:1]  # Close the loop\n",
    "            \n",
    "            # Plot values\n",
    "            ax.plot(angles, values, linewidth=2, linestyle='solid', label=f'Cluster {i}')\n",
    "            ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Fix axis to go in the right order and start at 12 o'clock\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    \n",
    "    # Draw axis lines for each angle and label\n",
    "    plt.xticks(angles[:-1], categories)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title('Regional Sales Patterns by Cluster', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hierarchical_results/hierarchical_regional_sales_radar.png')\n",
    "    plt.close()\n",
    "\n",
    "# Create a parallel coordinates plot for cluster visualization\n",
    "plt.figure(figsize=(15, 8))\n",
    "# Get a subset of data for parallel coordinates plot (can be too dense with all data)\n",
    "sample_size = min(500, len(df_sample))\n",
    "if len(df_sample) > sample_size:\n",
    "    sample_indices = np.random.choice(len(df_sample), sample_size, replace=False)\n",
    "    sample_df = df_sample.iloc[sample_indices].copy()\n",
    "else:\n",
    "    sample_df = df_sample.copy()\n",
    "\n",
    "# Standardize the data for parallel coordinates plot\n",
    "features_for_parallel = key_features[:7] if len(key_features) > 7 else key_features  # Limit to 7 features for readability\n",
    "sample_df_scaled = sample_df.copy()\n",
    "sample_df_scaled[features_for_parallel] = scaler.fit_transform(sample_df[features_for_parallel])\n",
    "\n",
    "# Create parallel coordinates plot\n",
    "pd.plotting.parallel_coordinates(\n",
    "    sample_df_scaled, 'Cluster', \n",
    "    cols=features_for_parallel,\n",
    "    colormap='viridis'\n",
    ")\n",
    "plt.title('Parallel Coordinates Plot of Video Game Clusters')\n",
    "plt.grid(False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('hierarchical_results/hierarchical_parallel_coordinates.png')\n",
    "plt.close()\n",
    "\n",
    "# Summarize the characteristics of each cluster\n",
    "print(\"\\nCluster Characteristics Summary:\")\n",
    "for cluster in range(optimal_k):\n",
    "    if cluster not in cluster_analysis.index:\n",
    "        continue  # Skip clusters that don't exist (possible if using 0-indexed)\n",
    "    \n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    # Get the top 5 distinctive features for this cluster (highest absolute z-scores)\n",
    "    if cluster in key_cluster_centers_scaled.index:\n",
    "        cluster_features = key_cluster_centers_scaled.loc[cluster].abs().sort_values(ascending=False)\n",
    "        top_features = cluster_features.head(5).index.tolist()\n",
    "        \n",
    "        for feature in top_features:\n",
    "            raw_value = key_cluster_centers.loc[cluster, feature]\n",
    "            scaled_value = key_cluster_centers_scaled.loc[cluster, feature]\n",
    "            direction = \"high\" if float(scaled_value) > 0 else \"low\"\n",
    "            print(f\"  - {feature}: {direction} ({raw_value:.2f}, z-score: {scaled_value:.2f})\")\n",
    "    \n",
    "    # Optional: Display additional cluster characteristics based on available metrics\n",
    "    if regional_cols and cluster in regional_means.index:\n",
    "        dominant_region = regional_means.loc[cluster].idxmax()\n",
    "        print(f\"  - Dominant sales region: {dominant_region}\")\n",
    "        \n",
    "    # Add any other relevant information about the clusters\n",
    "    if cluster in cluster_counts.index:\n",
    "        cluster_size = cluster_counts[cluster]\n",
    "        cluster_percentage = cluster_size / len(df_sample) * 100\n",
    "        print(f\"  - Size: {cluster_size} games ({cluster_percentage:.2f}% of dataset)\")\n",
    "\n",
    "# Sample titles from each cluster if title column exists\n",
    "if 'title' in df.columns:\n",
    "    print(\"\\nSample titles from each cluster:\")\n",
    "    for cluster in range(optimal_k):\n",
    "        if cluster in df_sample['Cluster'].values:  # Check if cluster exists in results\n",
    "            cluster_games = df_sample[df_sample['Cluster'] == cluster]\n",
    "            sample_size = min(5, len(cluster_games))\n",
    "            if sample_size > 0:\n",
    "                sample_titles = cluster_games['title'].sample(sample_size).tolist()\n",
    "                print(f\"\\nCluster {cluster} sample titles:\")\n",
    "                for title in sample_titles:\n",
    "                    print(f\"  - {title}\")\n",
    "\n",
    "# Create correlation matrix between original features and PCA components\n",
    "print(\"\\nCreating correlation matrix between features and PCA components...\")\n",
    "pca_components = pd.DataFrame(\n",
    "    pca.components_.T, \n",
    "    columns=[f'PC{i+1}' for i in range(2)],\n",
    "    index=features\n",
    ")\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.heatmap(pca_components, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "plt.title('Feature Correlation with Principal Components')\n",
    "plt.tight_layout()\n",
    "plt.savefig('hierarchical_results/hierarchical_pca_correlation.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nHierarchical clustering analysis complete. Results saved to 'hierarchical_results' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed video game sales dataset...\n",
      "Dataset shape: (4000, 24)\n",
      "Converting release_date to datetime and extracting year\n",
      "Converting last_update to datetime and extracting year\n",
      "High sales threshold (median): -0.54\n",
      "High sales prevalence: 48.25%\n",
      "Excluding columns: ['title', 'platform', 'genre', 'publisher', 'developer', 'high_sales', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'sales_per_year']\n",
      "\n",
      "Found non-numeric columns: ['rating_Poor', 'rating_Average', 'rating_Good', 'rating_Great', 'rating_Excellent']\n",
      "Dropping non-numeric columns for classification\n",
      "Selected 14 features for classification\n",
      "Features: ['critic_score', 'release_year', 'console_freq', 'genre_freq', 'publisher_freq', 'developer_freq', 'na_sales_ratio', 'jp_sales_ratio', 'pal_sales_ratio', 'game_age', 'release_date_year', 'release_date_month', 'last_update_year', 'last_update_month']\n",
      "Training set: 3000 samples\n",
      "Test set: 1000 samples\n",
      "\n",
      "Training a simple decision tree classifier...\n",
      "Decision tree accuracy: 0.9800\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       494\n",
      "           1       0.98      0.98      0.98       506\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.98      0.98      0.98      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n",
      "\n",
      "Top 10 Important Features:\n",
      "               Feature  Importance\n",
      "8      pal_sales_ratio    0.617162\n",
      "7       jp_sales_ratio    0.249834\n",
      "6       na_sales_ratio    0.109528\n",
      "4       publisher_freq    0.006118\n",
      "5       developer_freq    0.004174\n",
      "9             game_age    0.003276\n",
      "11  release_date_month    0.003010\n",
      "2         console_freq    0.002146\n",
      "1         release_year    0.001910\n",
      "3           genre_freq    0.001683\n",
      "\n",
      "Performing hyperparameter tuning for decision tree...\n",
      "Best parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best cross-validation score: 0.9729\n",
      "Optimized decision tree accuracy: 0.9800\n",
      "\n",
      "Classification Report (Optimized Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       494\n",
      "           1       0.98      0.98      0.98       506\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.98      0.98      0.98      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n",
      "\n",
      "Decision tree analysis complete. Results saved to 'decision_tree_results' directory.\n",
      "\n",
      "Example prediction:\n",
      "Sample features: {'critic_score': 80, 'release_year': 2018, 'console_freq': 5, 'genre_freq': 10, 'publisher_freq': 3}\n",
      "Prediction: High Sales\n",
      "Probability of high sales: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create directory for decision tree results\n",
    "os.makedirs('decision_tree_results', exist_ok=True)\n",
    "\n",
    "# Load the preprocessed data\n",
    "print(\"Loading preprocessed video game sales dataset...\")\n",
    "df = pd.read_csv('processed_data/vgchartz_processed.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Handle date columns if they exist\n",
    "date_columns = ['release_date', 'last_update']\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"Converting {col} to datetime and extracting year\")\n",
    "        try:\n",
    "            # Try to convert to datetime and extract year as a numeric feature\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            \n",
    "            # Drop original date column as it cannot be used directly\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        except:\n",
    "            print(f\"Error processing {col}, dropping it\")\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Need to create a target variable since this is sales data\n",
    "# Let's create a categorical target based on total sales\n",
    "# Create high/low sales binary classification (1 for high sales, 0 for low sales)\n",
    "# Using median as threshold to ensure balanced classes\n",
    "sales_median = df['total_sales'].median()\n",
    "df['high_sales'] = (df['total_sales'] > sales_median).astype(int)\n",
    "print(f\"High sales threshold (median): {sales_median:.2f}\")\n",
    "print(f\"High sales prevalence: {df['high_sales'].mean():.2%}\")\n",
    "\n",
    "# Select features for classification\n",
    "# Remove identifier columns and target\n",
    "exclude_cols = ['title', 'platform', 'genre', 'publisher', 'developer', 'high_sales', 'total_sales']\n",
    "\n",
    "# Also remove any features that are directly derived from total_sales to avoid data leakage\n",
    "derived_cols = ['na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'sales_per_year']\n",
    "exclude_cols.extend(derived_cols)\n",
    "\n",
    "print(f\"Excluding columns: {exclude_cols}\")\n",
    "features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Check for NaN values in the features and fill them\n",
    "nan_counts = df[features].isna().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(\"\\nFound NaN values in features. Filling with medians.\")\n",
    "    for col in features:\n",
    "        if nan_counts[col] > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Check for non-numeric columns\n",
    "non_numeric_cols = df[features].select_dtypes(exclude=['number']).columns.tolist()\n",
    "if non_numeric_cols:\n",
    "    print(f\"\\nFound non-numeric columns: {non_numeric_cols}\")\n",
    "    print(\"Dropping non-numeric columns for classification\")\n",
    "    features = [col for col in features if col not in non_numeric_cols]\n",
    "\n",
    "X = df[features].values\n",
    "y = df['high_sales'].values\n",
    "\n",
    "print(f\"Selected {len(features)} features for classification\")\n",
    "print(\"Features:\", features)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a decision tree with default parameters\n",
    "print(\"\\nTraining a simple decision tree classifier...\")\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = dt_clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Decision tree accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Save classification report to file\n",
    "with open('decision_tree_results/classification_report.txt', 'w') as f:\n",
    "    f.write(\"Classification Report for Decision Tree on Video Game Sales:\\n\")\n",
    "    f.write(str(report))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Low Sales', 'High Sales'],\n",
    "            yticklabels=['Low Sales', 'High Sales'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix for Decision Tree')\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree_results/confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': dt_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(feature_importance.head(min(10, len(features))))\n",
    "\n",
    "# Save feature importance to CSV\n",
    "feature_importance.to_csv('decision_tree_results/feature_importance.csv', index=False)\n",
    "\n",
    "# Plot feature importance (top 15)\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(min(15, len(features)))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "plt.title('Top Features by Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree_results/feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "print(\"\\nPerforming hyperparameter tuning for decision tree...\")\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Train model with best parameters\n",
    "best_dt = grid_search.best_estimator_\n",
    "best_dt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate optimized model\n",
    "y_pred_best = best_dt.predict(X_test_scaled)\n",
    "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Optimized decision tree accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Print classification report for optimized model\n",
    "print(\"\\nClassification Report (Optimized Model):\")\n",
    "best_report = classification_report(y_test, y_pred_best)\n",
    "print(best_report)\n",
    "\n",
    "# Save classification report for optimized model\n",
    "with open('decision_tree_results/optimized_classification_report.txt', 'w') as f:\n",
    "    f.write(\"Classification Report for Optimized Decision Tree on Video Game Sales:\\n\")\n",
    "    f.write(str(best_report))\n",
    "\n",
    "# Confusion matrix for optimized model\n",
    "best_conf_matrix = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(best_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Low Sales', 'High Sales'],\n",
    "            yticklabels=['Low Sales', 'High Sales'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix for Optimized Decision Tree')\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree_results/optimized_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize the optimized decision tree (if not too large)\n",
    "max_depth_for_visualization = 3  # Limit for visualization\n",
    "\n",
    "# If tree is too deep, create a simpler one just for visualization\n",
    "vis_dt = DecisionTreeClassifier(max_depth=max_depth_for_visualization, random_state=42)\n",
    "vis_dt.fit(X_train_scaled, y_train)\n",
    "tree_to_visualize = vis_dt\n",
    "tree_title = f\"Decision Tree (Limited to depth {max_depth_for_visualization} for visualization)\"\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(20, 15))\n",
    "plot_tree(\n",
    "    tree_to_visualize,\n",
    "    feature_names=features,\n",
    "    class_names=['Low Sales', 'High Sales'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(tree_title)\n",
    "plt.savefig('decision_tree_results/decision_tree_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Export text representation of the tree\n",
    "tree_text = export_text(\n",
    "    tree_to_visualize,\n",
    "    feature_names=features\n",
    ")\n",
    "\n",
    "with open('decision_tree_results/decision_tree_text.txt', 'w') as f:\n",
    "    f.write(tree_text)\n",
    "\n",
    "# Check if we have any sales ratio columns left after removing derived columns\n",
    "regional_ratio_cols = [col for col in features if '_sales_ratio' in col]\n",
    "if regional_ratio_cols:\n",
    "    # Create regional sales ratio plot by sales class\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sales_class_means = df.groupby('high_sales')[regional_ratio_cols].mean()\n",
    "    \n",
    "    sales_class_means.T.plot(kind='bar')\n",
    "    plt.title('Regional Sales Ratio by Sales Class')\n",
    "    plt.xlabel('Region')\n",
    "    plt.ylabel('Average Sales Ratio')\n",
    "    plt.legend(['Low Sales', 'High Sales'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('decision_tree_results/regional_sales_by_class.png')\n",
    "    plt.close()\n",
    "\n",
    "# Create feature distributions by sales class\n",
    "top_5_features = feature_importance.head(min(5, len(features)))['Feature'].tolist()\n",
    "for feature in top_5_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Using histplot without KDE to avoid errors\n",
    "    sns.histplot(data=df, x=feature, hue='high_sales', bins=20, element='step', kde=False)\n",
    "    plt.title(f'Distribution of {feature} by Sales Class')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'decision_tree_results/feature_distribution_{feature}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Create correlation matrix for features\n",
    "corr_matrix = df[features + ['high_sales']].corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', annot=True, fmt='.2f', \n",
    "            square=True, linewidths=.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree_results/correlation_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nDecision tree analysis complete. Results saved to 'decision_tree_results' directory.\")\n",
    "\n",
    "# Create a function for predicting new games' sales class\n",
    "def predict_sales_class(sample_data, model=best_dt, scaler=scaler, features=features):\n",
    "    \"\"\"\n",
    "    Predict sales class for a new video game.\n",
    "    \n",
    "    Parameters:\n",
    "    sample_data : dict\n",
    "        Dictionary with feature names and values\n",
    "    model : trained model\n",
    "        Trained decision tree model\n",
    "    scaler : fitted scaler\n",
    "        Fitted StandardScaler\n",
    "    features : list\n",
    "        List of feature names\n",
    "    \n",
    "    Returns:\n",
    "    prediction : int\n",
    "        0 for low sales, 1 for high sales\n",
    "    prob : float\n",
    "        Probability of high sales\n",
    "    \"\"\"\n",
    "    # Convert sample to array in the correct order\n",
    "    sample_array = np.array([sample_data.get(feature, 0) for feature in features]).reshape(1, -1)\n",
    "    \n",
    "    # Scale the sample\n",
    "    sample_scaled = scaler.transform(sample_array)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(sample_scaled)[0]\n",
    "    probability = model.predict_proba(sample_scaled)[0][1]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample prediction:\")\n",
    "# Create a sample (you should replace this with actual feature values)\n",
    "sample = {\n",
    "    'critic_score': 80,\n",
    "    'release_year': 2018,\n",
    "    'console_freq': 5,\n",
    "    'genre_freq': 10,\n",
    "    'publisher_freq': 3,\n",
    "    # Add other features as needed\n",
    "}\n",
    "\n",
    "# Print only features in the sample that match our feature list\n",
    "present_features = {k: v for k, v in sample.items() if k in features}\n",
    "print(f\"Sample features: {present_features}\")\n",
    "\n",
    "# Make a prediction\n",
    "try:\n",
    "    pred, prob = predict_sales_class(sample)\n",
    "    print(f\"Prediction: {'High Sales' if pred == 1 else 'Low Sales'}\")\n",
    "    print(f\"Probability of high sales: {prob:.2%}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error making prediction: {e}\")\n",
    "    print(\"This is just an example. You may need to provide values for all required features.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed video game sales dataset...\n",
      "Dataset shape: (4000, 24)\n",
      "Converting release_date to datetime and extracting year\n",
      "Converting last_update to datetime and extracting year\n",
      "High sales threshold (median): -0.54\n",
      "High sales prevalence: 48.25%\n",
      "Excluding columns: ['title', 'platform', 'genre', 'publisher', 'developer', 'high_sales', 'total_sales', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'sales_per_year']\n",
      "\n",
      "Found non-numeric columns: ['rating_Poor', 'rating_Average', 'rating_Good', 'rating_Great', 'rating_Excellent']\n",
      "Dropping non-numeric columns for classification\n",
      "Selected 14 features for classification\n",
      "Features: ['critic_score', 'release_year', 'console_freq', 'genre_freq', 'publisher_freq', 'developer_freq', 'na_sales_ratio', 'jp_sales_ratio', 'pal_sales_ratio', 'game_age', 'release_date_year', 'release_date_month', 'last_update_year', 'last_update_month']\n",
      "Training set: 3000 samples\n",
      "Test set: 1000 samples\n",
      "\n",
      "---- Gaussian Naive Bayes ----\n",
      "Gaussian Naive Bayes accuracy: 0.8440\n",
      "\n",
      "Classification Report (Gaussian Naive Bayes):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.99      0.86       494\n",
      "           1       0.98      0.71      0.82       506\n",
      "\n",
      "    accuracy                           0.84      1000\n",
      "   macro avg       0.87      0.85      0.84      1000\n",
      "weighted avg       0.87      0.84      0.84      1000\n",
      "\n",
      "\n",
      "---- Bernoulli Naive Bayes ----\n",
      "Bernoulli Naive Bayes accuracy: 0.5510\n",
      "\n",
      "Classification Report (Bernoulli Naive Bayes):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.88      0.66       494\n",
      "           1       0.67      0.23      0.34       506\n",
      "\n",
      "    accuracy                           0.55      1000\n",
      "   macro avg       0.60      0.55      0.50      1000\n",
      "weighted avg       0.60      0.55      0.50      1000\n",
      "\n",
      "\n",
      "---- Cross-validation ----\n",
      "Gaussian NB 5-fold CV accuracy: 0.8550  0.0061\n",
      "Bernoulli NB 5-fold CV accuracy: 0.5563  0.0196\n",
      "\n",
      "Top 10 Important Features (Gaussian NB):\n",
      "               Feature  Importance\n",
      "4       publisher_freq    0.752930\n",
      "12    last_update_year    0.503194\n",
      "9             game_age    0.304855\n",
      "1         release_year    0.304855\n",
      "10   release_date_year    0.304163\n",
      "5       developer_freq    0.256097\n",
      "13   last_update_month    0.174654\n",
      "3           genre_freq    0.159160\n",
      "7       jp_sales_ratio    0.120716\n",
      "11  release_date_month    0.114419\n",
      "\n",
      "---- Comparison with Decision Tree ----\n",
      "Decision Tree results loaded from file.\n",
      "\n",
      "Model Comparison:\n",
      "          Model  Accuracy  CV Accuracy       AUC\n",
      "0   Gaussian NB     0.844     0.855000  0.966035\n",
      "1  Bernoulli NB     0.551     0.556333  0.574389\n",
      "\n",
      "Best Naive Bayes model: Gaussian NB (Accuracy: 0.8440)\n",
      "\n",
      "Example prediction:\n",
      "Sample features: {'critic_score': 80, 'release_date_year': 2018, 'console_freq': 5, 'genre_freq': 10, 'publisher_freq': 3}\n",
      "Prediction: Low Sales\n",
      "Probability of high sales: 0.00%\n",
      "\n",
      "Generating feature distribution plots...\n",
      "\n",
      "Naive Bayes analysis complete. Results saved to 'naive_bayes_results' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Create directory for Naive Bayes results\n",
    "os.makedirs('naive_bayes_results', exist_ok=True)\n",
    "\n",
    "# Load the preprocessed data\n",
    "print(\"Loading preprocessed video game sales dataset...\")\n",
    "df = pd.read_csv('processed_data/vgchartz_processed.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Handle date columns if they exist\n",
    "date_columns = ['release_date', 'last_update']\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"Converting {col} to datetime and extracting year\")\n",
    "        try:\n",
    "            # Try to convert to datetime and extract year as a numeric feature\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            \n",
    "            # Drop original date column as it cannot be used directly\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        except:\n",
    "            print(f\"Error processing {col}, dropping it\")\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Create high/low sales binary classification (1 for high sales, 0 for low sales)\n",
    "# Using median as threshold to ensure balanced classes\n",
    "sales_median = df['total_sales'].median()\n",
    "df['high_sales'] = (df['total_sales'] > sales_median).astype(int)\n",
    "print(f\"High sales threshold (median): {sales_median:.2f}\")\n",
    "print(f\"High sales prevalence: {df['high_sales'].mean():.2%}\")\n",
    "\n",
    "# Select features for classification\n",
    "# Remove identifier columns and target\n",
    "exclude_cols = ['title', 'platform', 'genre', 'publisher', 'developer', 'high_sales', 'total_sales']\n",
    "\n",
    "# Also remove any features that are directly derived from total_sales to avoid data leakage\n",
    "derived_cols = ['na_sales', 'jp_sales', 'pal_sales', 'other_sales', 'sales_per_year']\n",
    "exclude_cols.extend(derived_cols)\n",
    "\n",
    "print(f\"Excluding columns: {exclude_cols}\")\n",
    "features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Check for NaN values in the features and fill them\n",
    "nan_counts = df[features].isna().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(\"\\nFound NaN values in features. Filling with medians.\")\n",
    "    for col in features:\n",
    "        if nan_counts[col] > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Check for non-numeric columns\n",
    "non_numeric_cols = df[features].select_dtypes(exclude=['number']).columns.tolist()\n",
    "if non_numeric_cols:\n",
    "    print(f\"\\nFound non-numeric columns: {non_numeric_cols}\")\n",
    "    print(\"Dropping non-numeric columns for classification\")\n",
    "    features = [col for col in features if col not in non_numeric_cols]\n",
    "\n",
    "X = df[features].values\n",
    "y = df['high_sales'].values\n",
    "\n",
    "print(f\"Selected {len(features)} features for classification\")\n",
    "print(\"Features:\", features)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale features - MinMaxScaler tends to work better with Naive Bayes than StandardScaler\n",
    "# as it preserves the distribution shape while bounding values\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -------------------------\n",
    "# Gaussian Naive Bayes\n",
    "# -------------------------\n",
    "print(\"\\n---- Gaussian Naive Bayes ----\")\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_gnb = gnb.predict(X_test_scaled)\n",
    "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "print(f\"Gaussian Naive Bayes accuracy: {accuracy_gnb:.4f}\")\n",
    "\n",
    "# Probability predictions for ROC curve\n",
    "y_pred_prob_gnb = gnb.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Gaussian Naive Bayes):\")\n",
    "gnb_report = classification_report(y_test, y_pred_gnb)\n",
    "print(gnb_report)\n",
    "\n",
    "# Save classification report to file\n",
    "with open('naive_bayes_results/gnb_classification_report.txt', 'w') as f:\n",
    "    f.write(\"Classification Report for Gaussian Naive Bayes on Video Game Sales:\\n\")\n",
    "    f.write(str(gnb_report))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_gnb = confusion_matrix(y_test, y_pred_gnb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_gnb, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Low Sales', 'High Sales'],\n",
    "            yticklabels=['Low Sales', 'High Sales'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix for Gaussian Naive Bayes')\n",
    "plt.tight_layout()\n",
    "plt.savefig('naive_bayes_results/gnb_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Bernoulli Naive Bayes \n",
    "# -------------------------\n",
    "print(\"\\n---- Bernoulli Naive Bayes ----\")\n",
    "# Bernoulli NB works with binary features, so we need to consider this fact\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_bnb = bnb.predict(X_test_scaled)\n",
    "accuracy_bnb = accuracy_score(y_test, y_pred_bnb)\n",
    "print(f\"Bernoulli Naive Bayes accuracy: {accuracy_bnb:.4f}\")\n",
    "\n",
    "# Probability predictions for ROC curve\n",
    "y_pred_prob_bnb = bnb.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Bernoulli Naive Bayes):\")\n",
    "bnb_report = classification_report(y_test, y_pred_bnb)\n",
    "print(bnb_report)\n",
    "\n",
    "# Save classification report to file\n",
    "with open('naive_bayes_results/bnb_classification_report.txt', 'w') as f:\n",
    "    f.write(\"Classification Report for Bernoulli Naive Bayes on Video Game Sales:\\n\")\n",
    "    f.write(str(bnb_report))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_bnb = confusion_matrix(y_test, y_pred_bnb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_bnb, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Low Sales', 'High Sales'],\n",
    "            yticklabels=['Low Sales', 'High Sales'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix for Bernoulli Naive Bayes')\n",
    "plt.tight_layout()\n",
    "plt.savefig('naive_bayes_results/bnb_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Cross-validation for both models\n",
    "# -------------------------\n",
    "print(\"\\n---- Cross-validation ----\")\n",
    "\n",
    "# Cross-validation for Gaussian NB\n",
    "cv_scores_gnb = cross_val_score(gnb, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Gaussian NB 5-fold CV accuracy: {cv_scores_gnb.mean():.4f}  {cv_scores_gnb.std():.4f}\")\n",
    "\n",
    "# Cross-validation for Bernoulli NB\n",
    "cv_scores_bnb = cross_val_score(bnb, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Bernoulli NB 5-fold CV accuracy: {cv_scores_bnb.mean():.4f}  {cv_scores_bnb.std():.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# ROC Curves and AUC\n",
    "# -------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# ROC for Gaussian NB\n",
    "fpr_gnb, tpr_gnb, _ = roc_curve(y_test, y_pred_prob_gnb)\n",
    "roc_auc_gnb = auc(fpr_gnb, tpr_gnb)\n",
    "plt.plot(fpr_gnb, tpr_gnb, label=f'Gaussian NB (AUC = {roc_auc_gnb:.3f})')\n",
    "\n",
    "# ROC for Bernoulli NB\n",
    "fpr_bnb, tpr_bnb, _ = roc_curve(y_test, y_pred_prob_bnb)\n",
    "roc_auc_bnb = auc(fpr_bnb, tpr_bnb)\n",
    "plt.plot(fpr_bnb, tpr_bnb, label=f'Bernoulli NB (AUC = {roc_auc_bnb:.3f})')\n",
    "\n",
    "# Reference line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Naive Bayes Classifiers')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('naive_bayes_results/roc_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Feature importance for Naive Bayes\n",
    "# -------------------------\n",
    "# For Naive Bayes, we can look at the difference in log probability between classes\n",
    "# This gives us an indication of how much each feature contributes to the classification\n",
    "\n",
    "# Function to calculate feature importance for Gaussian NB\n",
    "def compute_feature_importance_gnb(model, feature_names):\n",
    "    # Get the feature means for each class\n",
    "    theta_0 = model.theta_[0]  # Mean for class 0\n",
    "    theta_1 = model.theta_[1]  # Mean for class 1\n",
    "    \n",
    "    # Get the feature variances for each class\n",
    "    sigma_0 = model.var_[0]  # Variance for class 0\n",
    "    sigma_1 = model.var_[1]  # Variance for class 1\n",
    "    \n",
    "    # Calculate the absolute difference in means, normalized by variance\n",
    "    # This gives a measure of how discriminative each feature is\n",
    "    importance = np.abs(theta_1 - theta_0) / np.sqrt((sigma_0 + sigma_1) / 2)\n",
    "    \n",
    "    # Create DataFrame with feature names and importance scores\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Calculate feature importance for Gaussian NB\n",
    "gnb_feature_importance = compute_feature_importance_gnb(gnb, features)\n",
    "\n",
    "print(\"\\nTop 10 Important Features (Gaussian NB):\")\n",
    "print(gnb_feature_importance.head(min(10, len(features))))\n",
    "\n",
    "# Save feature importance to CSV\n",
    "gnb_feature_importance.to_csv('naive_bayes_results/gnb_feature_importance.csv', index=False)\n",
    "\n",
    "# Plot feature importance (top 15)\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features_gnb = gnb_feature_importance.head(min(15, len(features)))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_features_gnb)\n",
    "plt.title('Top 15 Features by Importance (Gaussian NB)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('naive_bayes_results/gnb_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Compare with decision tree results if available\n",
    "# -------------------------\n",
    "print(\"\\n---- Comparison with Decision Tree ----\")\n",
    "\n",
    "# Create comparison table with the Naive Bayes models\n",
    "comparison_data = {\n",
    "    'Model': ['Gaussian NB', 'Bernoulli NB'],\n",
    "    'Accuracy': [accuracy_gnb, accuracy_bnb],\n",
    "    'CV Accuracy': [cv_scores_gnb.mean(), cv_scores_bnb.mean()],\n",
    "    'AUC': [roc_auc_gnb, roc_auc_bnb]\n",
    "}\n",
    "\n",
    "# Try to load Decision Tree results from file\n",
    "try:\n",
    "    with open('decision_tree_results/optimized_classification_report.txt', 'r') as f:\n",
    "        dt_report = f.read()\n",
    "    print(\"Decision Tree results loaded from file.\")\n",
    "    \n",
    "    # Add Decision Tree to comparison if we can extract accuracy\n",
    "    # This is a simplistic approach; in a real application you might parse the report more carefully\n",
    "    import re\n",
    "    accuracy_match = re.search(r'accuracy\\s*:\\s*([\\d\\.]+)', dt_report)\n",
    "    if accuracy_match:\n",
    "        dt_accuracy = float(accuracy_match.group(1))\n",
    "        comparison_data['Model'].insert(0, 'Decision Tree')\n",
    "        comparison_data['Accuracy'].insert(0, dt_accuracy)\n",
    "        comparison_data['CV Accuracy'].insert(0, None)\n",
    "        comparison_data['AUC'].insert(0, None)\n",
    "except FileNotFoundError:\n",
    "    print(\"Decision Tree results file not found. Comparison will only include Naive Bayes models.\")\n",
    "\n",
    "# Create and display comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.fillna('Not available')\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Save comparison to file\n",
    "comparison_df.to_csv('naive_bayes_results/model_comparison.csv', index=False)\n",
    "\n",
    "# -------------------------\n",
    "# Create prediction function\n",
    "# -------------------------\n",
    "# Determine the best Naive Bayes model\n",
    "best_nb_model = gnb if accuracy_gnb >= accuracy_bnb else bnb\n",
    "best_nb_name = \"Gaussian NB\" if accuracy_gnb >= accuracy_bnb else \"Bernoulli NB\"\n",
    "best_accuracy = accuracy_gnb if accuracy_gnb >= accuracy_bnb else accuracy_bnb\n",
    "print(f\"\\nBest Naive Bayes model: {best_nb_name} (Accuracy: {best_accuracy:.4f})\")\n",
    "\n",
    "def predict_sales_class_nb(sample_data, model=best_nb_model, scaler=scaler, features=features):\n",
    "    \"\"\"\n",
    "    Predict sales class for a new video game using Naive Bayes.\n",
    "    \n",
    "    Parameters:\n",
    "    sample_data : dict\n",
    "        Dictionary with feature names and values\n",
    "    model : trained model\n",
    "        Trained Naive Bayes model\n",
    "    scaler : fitted scaler\n",
    "        Fitted scaler\n",
    "    features : list\n",
    "        List of feature names\n",
    "    \n",
    "    Returns:\n",
    "    prediction : int\n",
    "        0 for low sales, 1 for high sales\n",
    "    prob : float\n",
    "        Probability of high sales\n",
    "    \"\"\"\n",
    "    # Convert sample to array in the correct order\n",
    "    sample_array = np.array([sample_data.get(feature, 0) for feature in features]).reshape(1, -1)\n",
    "    \n",
    "    # Scale the sample\n",
    "    sample_scaled = scaler.transform(sample_array)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(sample_scaled)[0]\n",
    "    probability = model.predict_proba(sample_scaled)[0][1]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample prediction:\")\n",
    "# Create a sample (you should replace this with actual feature values)\n",
    "sample = {\n",
    "    'critic_score': 80,\n",
    "    'release_date_year': 2018,\n",
    "    'console_freq': 5,\n",
    "    'genre_freq': 10,\n",
    "    'publisher_freq': 3,\n",
    "    # Add other features as needed\n",
    "}\n",
    "\n",
    "# Print only features in the sample that are in our feature list\n",
    "present_features = {k: v for k, v in sample.items() if k in features}\n",
    "print(f\"Sample features: {present_features}\")\n",
    "\n",
    "# Make a prediction\n",
    "try:\n",
    "    pred, prob = predict_sales_class_nb(sample)\n",
    "    print(f\"Prediction: {'High Sales' if pred == 1 else 'Low Sales'}\")\n",
    "    print(f\"Probability of high sales: {prob:.2%}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error making prediction: {e}\")\n",
    "    print(\"This is just an example. You may need to provide values for all required features.\")\n",
    "\n",
    "# Additional analysis: Feature distributions by sales class\n",
    "print(\"\\nGenerating feature distribution plots...\")\n",
    "top_5_features = gnb_feature_importance.head(min(5, len(features)))['Feature'].tolist()\n",
    "for feature in top_5_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x=feature, hue='high_sales', bins=20, element='step', kde=False)\n",
    "    plt.title(f'Distribution of {feature} by Sales Class')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'naive_bayes_results/feature_distribution_{feature}.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nNaive Bayes analysis complete. Results saved to 'naive_bayes_results' directory.\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
